Setting up Kafka 3.9 without Zookeeper involves using the **KRaft (Kafka Raft)** mode, which eliminates the need for Zookeeper. Below are the steps and commands to set up a 12-node Kafka cluster, manage topics, and perform health checks.

---

### **1. Setting Up Kafka 3.9 Without Zookeeper**
1. **Download Kafka 3.9**:
   ```bash
   wget https://downloads.apache.org/kafka/3.9.0/kafka_2.13-3.9.0.tgz
   tar -xzf kafka_2.13-3.9.0.tgz
   cd kafka_2.13-3.9.0
   ```

2. **Configure Kafka in KRaft Mode**:
   - Edit the `server.properties` file for each node:
     ```bash
     nano config/server.properties
     ```
     Add the following configurations:
     ```properties
     process.roles=broker,controller
     node.id=<unique_node_id> # Replace with a unique ID for each node
     controller.quorum.voters=1@node1:9093,2@node2:9093,...,12@node12:9093
     listeners=PLAINTEXT://:9092,CONTROLLER://:9093
     listener.security.protocol.map=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
     log.dirs=/tmp/kraft-combined-logs
     ```

3. **Format the Storage Directory**:
   Run this command on each node to initialize the storage:
   ```bash
   bin/kafka-storage.sh format -t <cluster_id> -c config/server.properties
   ```

4. **Start Kafka Brokers**:
   Start each broker using:
   ```bash
   bin/kafka-server-start.sh config/server.properties
   ```

---

### **2. Commands for Managing Topics**
- **Create a Topic**:
  ```bash
  bin/kafka-topics.sh --bootstrap-server <broker_address>:9092 --create --topic <topic_name> --partitions <num_partitions> --replication-factor <replication_factor>
  ```

- **List Topics**:
  ```bash
  bin/kafka-topics.sh --bootstrap-server <broker_address>:9092 --list
  ```

- **Describe a Topic**:
  ```bash
  bin/kafka-topics.sh --bootstrap-server <broker_address>:9092 --describe --topic <topic_name>
  ```

- **Delete a Topic**:
  ```bash
  bin/kafka-topics.sh --bootstrap-server <broker_address>:9092 --delete --topic <topic_name>
  ```

---

### **3. Commands for Health Checks**
- **Check Broker Health**:
  Use the `kafka-broker-api-versions.sh` tool:
  ```bash
  bin/kafka-broker-api-versions.sh --bootstrap-server <broker_address>:9092
  ```

- **Check Cluster Metadata**:
  ```bash
  bin/kafka-metadata-shell.sh --bootstrap-server <broker_address>:9092
  ```

- **Monitor Logs**:
  Check the logs for errors or warnings:
  ```bash
  tail -f /tmp/kraft-combined-logs/server.log
  ```

- **Custom Health Check Script**:
  Create a script to verify broker availability:
  ```bash
  curl -s <broker_address>:9092 | grep "Kafka"
  ```

---

Let me know if you'd like further assistance with any of these steps! ðŸš€

Each configuration impacts Kafka's performance in specific ways, particularly in terms of efficiency, scalability, fault tolerance, and communication. Here's how these settings influence performance:

---

### **1. `process.roles=broker,controller`**
- By combining roles (`broker,controller`), each node handles both data storage and cluster management tasks. 
- **Impact**: While this setup simplifies configuration, it can lead to resource contention, especially in high-load scenarios. For large-scale deployments, separating the roles across nodes can optimize performance and reduce bottlenecks.

---

### **2. `node.id=<unique_node_id>`**
- The unique ID ensures that nodes are correctly identified in the cluster.
- **Impact**: Properly assigning IDs avoids conflicts that might disrupt cluster coordination or lead to metadata inconsistency.

---

### **3. `controller.quorum.voters`**
- Defining quorum voters determines the fault tolerance and availability of the cluster.
- **Impact**: Having a larger quorum (e.g., 12 nodes) increases fault tolerance but may lead to slightly higher latency during metadata updates, as decisions require agreement from a majority of nodes. For critical environments, the trade-off between fault tolerance and performance is key.

---

### **4. `listeners`**
- Separating `PLAINTEXT` for client traffic and `CONTROLLER` for internal communication helps distribute the load.
- **Impact**: By isolating controller traffic from client traffic, you reduce the risk of one impacting the other, leading to more stable and predictable performance.

---

### **5. `listener.security.protocol.map`**
- The choice of security protocols influences communication efficiency.
- **Impact**: Using `PLAINTEXT` (as in this example) avoids the overhead of encryption, leading to better performance. However, in production environments, secure protocols like `SSL` might slightly reduce throughput due to encryption overhead but are critical for data security.

---

### **6. `log.dirs`**
- The directory where logs are stored determines how Kafka handles disk I/O operations.
- **Impact**: Configuring multiple log directories across high-performance disks can significantly improve throughput and fault tolerance. Disk failures in one directory won't lead to complete data loss if others are available.

---

### **Overall Considerations**:
- These configurations strike a balance between simplicity and performance. Proper hardware (e.g., CPUs, memory, disk speed) and careful tuning (e.g., adjusting the number of partitions, replication factor) can further optimize Kafka's performance in your 12-node setup.

Feel free to share if you'd like deeper guidance on optimizing specific aspects! ðŸš€



To productionize your Kafka setup and include **Role-Based Access Control (RBAC)**, you'll need to focus on security, scalability, monitoring, and access management. Here's a step-by-step guide:

---

### **1. Productionizing Kafka Setup**
#### **a. Hardware and Network Configuration**
- Use high-performance disks (e.g., SSDs) for Kafka logs to improve throughput.
- Ensure sufficient memory and CPU resources for each node.
- Set up a reliable network with low latency and high bandwidth between nodes.

#### **b. Secure Communication**
- Enable **SSL/TLS** for encrypted communication between brokers, clients, and controllers:
  ```properties
  listeners=SSL://:9092,CONTROLLER://:9093
  ssl.keystore.location=/path/to/keystore.jks
  ssl.keystore.password=<password>
  ssl.truststore.location=/path/to/truststore.jks
  ssl.truststore.password=<password>
  ```
- Update `listener.security.protocol.map` to include `SSL`.

#### **c. Data Durability**
- Configure replication for fault tolerance:
  ```properties
  default.replication.factor=3
  min.insync.replicas=2
  ```
- Use multiple log directories to distribute data across disks:
  ```properties
  log.dirs=/data/kafka-logs1,/data/kafka-logs2
  ```

#### **d. Monitoring and Metrics**
- Integrate monitoring tools like **Prometheus** and **Grafana** for real-time metrics.
- Enable JMX (Java Management Extensions) for Kafka:
  ```bash
  export KAFKA_JMX_OPTS="-Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=9999 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false"
  ```

#### **e. Backup and Recovery**
- Set up periodic backups of Kafka logs and configurations.
- Test recovery procedures to ensure minimal downtime during failures.

---

### **2. Implementing RBAC**
RBAC ensures that only authorized users can perform specific actions on Kafka resources. Here's how to set it up:

#### **a. Enable RBAC**
- If you're using **Confluent Kafka**, RBAC is built-in. For open-source Kafka, you can use tools like **Apache Ranger** or **Kafka UI**.
- Configure the RBAC provider (e.g., LDAP, OAuth2):
  ```properties
  authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer
  super.users=User:admin
  ```

#### **b. Define Roles and Permissions**
- Create roles based on responsibilities (e.g., Admin, Producer, Consumer).
- Assign permissions using Access Control Lists (ACLs):
  ```bash
  # Allow a producer to write to a topic
  bin/kafka-acls.sh --bootstrap-server <broker_address>:9092 --add --allow-principal User:<username> --operation Write --topic <topic_name>

  # Allow a consumer to read from a topic
  bin/kafka-acls.sh --bootstrap-server <broker_address>:9092 --add --allow-principal User:<username> --operation Read --topic <topic_name>
  ```

#### **c. Authentication**
- Use **SASL** (Simple Authentication and Security Layer) for user authentication:
  ```properties
  sasl.enabled.mechanisms=PLAIN
  sasl.mechanism.inter.broker.protocol=PLAIN
  ```
- Configure a JAAS file for credentials:
  ```bash
  nano /etc/kafka/kafka_server_jaas.conf
  ```
  Example:
  ```plaintext
  KafkaServer {
      org.apache.kafka.common.security.plain.PlainLoginModule required
      username="admin"
      password="admin-secret";
  };
  ```

#### **d. Audit Logging**
- Enable audit logging to track access and changes:
  ```properties
  log4j.appender.audit=org.apache.log4j.RollingFileAppender
  log4j.appender.audit.File=/var/log/kafka/audit.log
  log4j.appender.audit.layout=org.apache.log4j.PatternLayout
  log4j.appender.audit.layout.ConversionPattern=%d{ISO8601} %p %c: %m%n
  ```

---

### **3. Best Practices**
- Regularly review and update ACLs to ensure least privilege.
- Rotate credentials and SSL certificates periodically.
- Automate deployments using tools like **Ansible** or **Terraform**.
- Perform load testing to validate the cluster's capacity under production workloads.

---

Let me know if you'd like help with any specific part of this process! ðŸš€