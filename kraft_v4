On aws using terraform create a 12 node of kafka kraft 3.9 cluster
Each az should have 4 nodes, kafka clients should run on 12 nodes and kafka controllers on 3 nodes.
Use sasl ssl and with all the connections
Use 6 ebs 1tb each call them local_disks 01 to 06
Application should be installed on local_disks01 and other app data on rest of the disks
Install java python and kafka from s3
Should have envs dev qa staging and prods
Should also have mirrormake running on separate cluster of 4 nodes
Keep all module and properties files in structured and properties files separate and dont update with with CAT EOF, HAVE THEM SEPERATE
Provide detailed infostructure and test plan and readme


# Kafka KRaft Cluster Infrastructure on AWS with Terraform

This document provides a complete, production-ready Terraform-based infrastructure for deploying a 12-node Apache Kafka 3.9.0 KRaft cluster on AWS, using SASL/SSL for authentication and encryption across all connections (including controller and inter-broker). The setup includes:

- **12 Kafka brokers** in KRaft mode, distributed across 3 Availability Zones (AZs) with 4 nodes per AZ (e.g., us-east-1a, us-east-1b, us-east-1c).
- **3 dedicated controller roles** assigned to one node per AZ (brokers can act as controllers in KRaft; here, we designate 3 brokers as combined controller-brokers for quorum).
- **SASL/SSL enabled** for all internal (inter-broker, controller) and external (client) connections. Uses SCRAM-SHA-256 for auth and mutual TLS.
- **Storage**: Each node has 6 x 1TB EBS gp3 volumes (named `local_disks01` to `local_disks06` via tags). `local_disks01` mounts to `/app` for application binaries; the remaining 5 mount to `/data/kafka{01-05}` for Kafka data/logs.
- **Installation**: Java 11 (OpenJDK), Python 3.12, and Kafka 3.9.0 downloaded from private S3 buckets during instance bootstrap (via user data/cloud-init).
- **Environments**: Support for `dev`, `qa`, `staging`, `prod` via separate `.tfvars` files and Terraform workspaces.
- **MirrorMaker2 (MM2)**: Separate 4-node cluster (1 per AZ + 1 extra for redundancy) for replication, also with SASL/SSL.
- **Networking**: VPC with public/private subnets, security groups for Kafka ports (9092 client, 9093 controller, 9094 inter-broker), NLB for external access.
- **Monitoring/Logging**: Basic CloudWatch integration; assumes external tools like Prometheus for advanced metrics.
- **Modular Structure**: Reusable modules for EC2 instances, EBS, networking, etc. Properties files are separate YAML/INI files (not inlined with `cat <<EOF`); referenced via `templatefile()` in user data.
- **Security**: IAM roles for S3 access, KMS for SSL keys, least-privilege policies.

## Prerequisites
- AWS account with admin privileges.
- Terraform 1.5+ installed.
- AWS CLI configured (`aws configure`).
- Private S3 buckets: `kafka-artifacts-${env}` containing:
  - `kafka_3.9.0.tgz` (official Kafka tarball).
  - `openjdk-11.0.XX_linux-x64_bin.tar.gz`.
  - `python-3.12.XX-amd64.tgz`.
  - SSL certs: `kafka.server.keystore.jks`, `kafka.client.truststore.jks`, `ca-cert.pem` (generated via `keytool` or cert-manager).
- Generate JAAS config and Kafka properties offline (samples provided below).
- Git for version control.

## Directory Structure
```
kafka-terraform/
├── README.md                 # This file
├── versions.tf               # Provider constraints
├── backend.tf                # S3 backend for state
├── environments/             # Env-specific configs
│   ├── dev.tfvars
│   ├── qa.tfvars
│   ├── staging.tfvars
│   └── prod.tfvars
├── modules/                  # Reusable Terraform modules
│   ├── vpc/                  # VPC + subnets + NLB
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── outputs.tf
│   ├── ec2-broker/           # Kafka broker EC2 module
│   │   ├── main.tf
│   │   ├── user-data.tpl     # Cloud-init template
│   │   ├── variables.tf
│   │   └── outputs.tf
│   ├── ec2-controller/       # Controller-specific (subset of broker)
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── outputs.tf
│   ├── ebs-volumes/          # EBS attachment module
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── outputs.tf
│   ├── mm2-cluster/          # MirrorMaker2 cluster module
│   │   ├── main.tf
│   │   ├── user-data.tpl
│   │   ├── variables.tf
│   │   └── outputs.tf
│   └── s3-artifacts/         # S3 setup for installs
│       ├── main.tf
│       ├── variables.tf
│       └── outputs.tf
├── configs/                  # Separate config files (YAML/INI)
│   ├── kafka/                # Kafka properties
│   │   ├── server.properties # Broker config
│   │   ├── controller.properties # Controller-specific overrides
│   │   ├── client.properties # Client config
│   │   └── mm2.properties    # MirrorMaker2 config
│   ├── security/             # Auth/SSL
│   │   ├── kafka_jaas.conf   # JAAS for SASL
│   │   └── ssl.properties    # SSL keystore props
│   └── install/              # Bootstrap scripts
│       ├── install-kafka.sh
│       ├── install-java.sh
│       └── install-python.sh
├── main.tf                   # Root module: orchestrates everything
├── variables.tf              # Global vars
├── outputs.tf                # Outputs (e.g., NLB DNS)
├── test-plan.md              # Testing guide
└── .gitignore                # Standard ignores
```

## Key Terraform Files

### versions.tf
```hcl
terraform {
  required_version = ">= 1.5.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  backend "s3" {
    bucket         = "terraform-state-${var.environment}"
    key            = "kafka/terraform.tfstate"
    region         = "us-east-1"
    dynamodb_table = "terraform-locks"
  }
}
```

### backend.tf
```hcl
# Empty; backend config is in versions.tf for simplicity
```

### variables.tf (Global)
```hcl
variable "environment" {
  description = "Environment (dev, qa, staging, prod)"
  type        = string
  validation {
    condition     = contains(["dev", "qa", "staging", "prod"], var.environment)
    error_message = "Environment must be dev, qa, staging, or prod."
  }
}

variable "region" {
  description = "AWS Region"
  type        = string
  default     = "us-east-1"
}

variable "vpc_cidr" {
  description = "VPC CIDR"
  type        = string
  default     = "10.0.0.0/16"
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "m5.4xlarge"  # Adjust for env (e.g., smaller for dev)
}

variable "s3_bucket_prefix" {
  description = "S3 bucket prefix for artifacts"
  type        = string
  default     = "kafka-artifacts"
}

# ... other vars: key_name, kafka_version="3.9.0", etc.
```

### environments/dev.tfvars
```hcl
environment = "dev"
instance_type = "m5.large"  # Smaller for dev
num_azs = 3
```

### environments/prod.tfvars
```hcl
environment = "prod"
instance_type = "m5.8xlarge"
auto_scaling = true  # Enable ASG for prod
```

### modules/vpc/main.tf
```hcl
resource "aws_vpc" "main" {
  cidr_block = var.vpc_cidr
  tags = {
    Name        = "${var.environment}-kafka-vpc"
    Environment = var.environment
  }
}

resource "aws_subnet" "private" {
  count = var.num_azs
  vpc_id            = aws_vpc.main.id
  cidr_block        = cidrsubnet(aws_vpc.main.cidr_block, 8, count.index)
  availability_zone = data.aws_availability_zones.available.names[count.index]
  tags = {
    Name        = "${var.environment}-private-${count.index + 1}"
    Environment = var.environment
  }
}

resource "aws_network_load_balancer" "kafka_nlb" {
  name               = "${var.environment}-kafka-nlb"
  internal           = false
  load_balancer_type = "network"
  subnets            = aws_subnet.public[*].id  # Assume public subnets too

  # Target groups for 9092 (client), 9093 (controller)
}

# Security Groups
resource "aws_security_group" "kafka_sg" {
  vpc_id = aws_vpc.main.id
  ingress {
    from_port   = 9092  # Client
    to_port     = 9092
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr]
  }
  ingress {
    from_port   = 9093  # Controller
    to_port     = 9093
    protocol    = "tcp"
    self        = true   # Inter-controller
  }
  ingress {
    from_port   = 9094  # Inter-broker
    to_port     = 9094
    protocol    = "tcp"
    self        = true
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
  tags = {
    Name = "${var.environment}-kafka-sg"
  }
}
```

### modules/ec2-broker/main.tf
```hcl
data "template_file" "user_data" {
  template = file("${path.module}/user-data.tpl")
  vars = {
    s3_bucket      = var.s3_bucket
    kafka_version  = var.kafka_version
    environment    = var.environment
    # Pass props paths for template
  }
}

resource "aws_launch_template" "broker_lt" {
  name_prefix   = "${var.environment}-broker-"
  image_id      = data.aws_ami.amazon_linux.id.id  # Amazon Linux 2023
  instance_type = var.instance_type
  key_name      = var.key_name
  vpc_security_group_ids = [var.kafka_sg_id]

  user_data = base64encode(data.template_file.user_data.rendered)

  block_device_mappings {
    device_name = "/dev/xvda"
    ebs {
      volume_size = 20  # Root
      volume_type = "gp3"
    }
  }

  tag_specifications {
    resource_type = "instance"
    tags = {
      Name        = "${var.environment}-broker-${count.index}"
      Role        = "broker"
      Environment = var.environment
    }
  }
}

resource "aws_autoscaling_group" "broker_asg" {
  name                = "${var.environment}-broker-asg"
  vpc_zone_identifier = var.private_subnet_ids
  min_size            = 12
  max_size            = 12
  desired_capacity    = 12
  health_check_type   = "ELB"
  launch_template {
    id      = aws_launch_template.broker_lt.id
    version = "$Latest"
  }

  # Distribute 4 per AZ
  tag {
    key                 = "AvailabilityZone"
    value               = "balanced"
    propagate_at_launch = true
  }
}

# Designate 3 as controllers (one per AZ) via tag
resource "aws_instance" "controller" {
  count                       = 3
  ami                         = data.aws_ami.amazon_linux.id.id
  instance_type               = var.instance_type
  subnet_id                   = var.private_subnet_ids[count.index % 3]
  vpc_security_group_ids      = [var.kafka_sg_id]
  associate_public_ip_address = false
  key_name                    = var.key_name
  user_data                   = base64encode(templatefile("${path.module}/user-data.tpl", { role = "controller" }))  # Override for controller

  tags = {
    Name        = "${var.environment}-controller-${count.index + 1}"
    Role        = "controller"
    Controller  = "true"
    Environment = var.environment
  }
}
```

### modules/ec2-broker/user-data.tpl (Cloud-Init Template)
```yaml
#cloud-config
package_update: true
package_upgrade: true

write_files:
  - path: /etc/kafka/server.properties
    content: |
      ${templatefile("${path.module}/../../configs/kafka/server.properties", { bootstrap_servers = var.bootstrap_servers, ssl_enabled = true })}
    permissions: '0644'
    owner: kafka:kafka

  - path: /etc/kafka/controller.properties
    content: |
      ${templatefile("${path.module}/../../configs/kafka/controller.properties", { node_id = var.node_id })}
    permissions: '0644'
    owner: kafka:kafka

  - path: /etc/kafka/kafka_jaas.conf
    content: |
      ${file("${path.module}/../../configs/security/kafka_jaas.conf")}
    permissions: '0600'
    owner: kafka:kafka

  - path: /etc/kafka/ssl.properties
    content: |
      ${file("${path.module}/../../configs/security/ssl.properties")}
    permissions: '0644'
    owner: kafka:kafka

runcmd:
  - aws s3 cp s3://${s3_bucket}/kafka_${kafka_version}.tgz /tmp/
  - tar -xzf /tmp/kafka_${kafka_version}.tgz -C /app/kafka --strip-components=1
  - chown -R kafka:kafka /app/kafka
  - aws s3 cp s3://${s3_bucket}/openjdk-*.tar.gz /tmp/
  - tar -xzf /tmp/openjdk-*.tar.gz -C /usr/local/
  - ln -s /usr/local/jdk* /usr/local/java
  - aws s3 cp s3://${s3_bucket}/python-*.tgz /tmp/
  - tar -xzf /tmp/python-*.tgz -C /usr/local/
  - ln -s /usr/local/Python* /usr/local/python
  - /app/install/install-kafka.sh  # Mount/format EBS first
  - /app/install/install-java.sh
  - /app/install/install-python.sh
  - systemctl enable kafka
  - systemctl start kafka

# EBS mounting (via script)
- mkfs.xfs /dev/nvme1n1 && mount /dev/nvme1n1 /app  # local_disks01
- for i in {2..6}; do mkfs.xfs /dev/nvme${i}n1 && mkdir -p /data/kafka$((i-1)) && mount /dev/nvme${i}n1 /data/kafka$((i-1)); done
```

### modules/ebs-volumes/main.tf
```hcl
resource "aws_ebs_volume" "local_disks" {
  count             = 6
  availability_zone = var.az
  size              = 1024  # 1TB
  type              = "gp3"
  encrypted         = true
  kms_key_id        = var.kms_key_id

  tags = {
    Name        = "${var.environment}-local_disks${format("%02d", count.index + 1)}"
    Environment = var.environment
    InstanceId  = var.instance_id  # Attach to specific instance
  }
}

resource "aws_volume_attachment" "ebs_att" {
  count       = 6
  device_name = "/dev/nvme${count.index + 1}n1"  # NVMe mapping
  volume_id   = aws_ebs_volume.local_disks[count.index].id
  instance_id = var.instance_id
}
```

### configs/kafka/server.properties (Separate File)
```properties
# Broker config for KRaft
process.roles=broker,controller  # For controller nodes; else just 'broker'
node.id=${node_id}
controller.quorum.voters=1@broker-1.internal:9093,2@broker-2.internal:9093,3@broker-3.internal:9093

listeners=CONTROLLER://0.0.0.0:9093,SSL://0.0.0.0:9092,SASL_SSL://0.0.0.0:9094
advertised.listeners=SSL://broker-${node_id}.${var.environment}.internal:9092,SASL_SSL://broker-${node_id}.${var.environment}.internal:9094
listener.security.protocol.map=CONTROLLER:SASL_SSL,SSL:SASL_SSL,SASL_SSL:SASL_SSL

ssl.keystore.location=/etc/kafka/kafka.server.keystore.jks
ssl.keystore.password=changeit
ssl.key.password=changeit
ssl.truststore.location=/etc/kafka/kafka.client.truststore.jks
ssl.truststore.password=changeit

sasl.mechanism.inter.broker.protocol=SCRAM-SHA-256
sasl.enabled.mechanisms=SCRAM-SHA-256
inter.broker.listener.name=SASL_SSL

log.dirs=/data/kafka01,/data/kafka02,/data/kafka03,/data/kafka04,/data/kafka05
num.network.threads=8
num.io.threads=16
# ... other standard props: auto.create.topics.enable=false, etc.
```

### configs/kafka/client.properties
```properties
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
ssl.truststore.location=/etc/kafka/kafka.client.truststore.jks
ssl.truststore.password=changeit
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="client" password="client-secret";
bootstrap.servers=${bootstrap_servers}
```

### configs/kafka/controller.properties
```properties
# Overrides for controllers
process.roles=controller
controller.listener.names=CONTROLLER
listeners=CONTROLLER://0.0.0.0:9093
```

### configs/kafka/mm2.properties
```properties
# MirrorMaker2 config
clusters=source,target
source.bootstrap.servers=source-brokers:9092
target.bootstrap.servers=target-brokers:9092
# Replication policies, etc.
security.protocol=SASL_SSL
# ... similar SSL/SASL as above
```

### configs/security/kafka_jaas.conf
```conf
KafkaServer {
  org.apache.kafka.common.security.scram.ScramLoginModule required
  username="admin"
  password="admin-secret"
  mechanism="SCRAM-SHA-256";
};

Client {
  org.apache.kafka.common.security.scram.ScramLoginModule required
  username="client"
  password="client-secret";
};
```

### configs/security/ssl.properties
```properties
ssl.protocol=TLSv1.2
ssl.enabled.protocols=TLSv1.2,TLSv1.3
ssl.endpoint.identification.algorithm=
```

### modules/mm2-cluster/main.tf (Similar to Broker, but 4 nodes)
```hcl
# Analogous to ec2-broker, but:
resource "aws_autoscaling_group" "mm2_asg" {
  # ... min_size=4, etc.
  launch_template {
    # User data with mm2.properties
  }
}

# User data downloads Kafka again, runs MM2 as service
```

### modules/s3-artifacts/main.tf
```hcl
resource "aws_s3_bucket" "artifacts" {
  bucket = "${var.s3_bucket_prefix}-${var.environment}"
}

resource "aws_s3_object" "kafka" {
  bucket = aws_s3_bucket.artifacts.id
  key    = "kafka_3.9.0.tgz"
  source = "path/to/local/kafka_3.9.0.tgz"  # Upload locally
  # Similar for Java/Python/SSL files
}

resource "aws_s3_bucket_policy" "ec2_access" {
  bucket = aws_s3_bucket.artifacts.id
  policy = jsonencode({
    Statement = [{
      Effect    = "Allow"
      Principal = { AWS = var.ec2_role_arn }
      Action    = ["s3:GetObject"]
      Resource  = "${aws_s3_bucket.artifacts.arn}/*"
    }]
  })
}
```

### main.tf (Root)
```hcl
provider "aws" {
  region = var.region
}

module "s3_artifacts" {
  source = "./modules/s3-artifacts"
  environment = var.environment
}

module "vpc" {
  source = "./modules/vpc"
  environment = var.environment
  vpc_cidr = var.vpc_cidr
  num_azs = 3
}

module "ebs_volumes" {
  source = "./modules/ebs-volumes"
  for_each = { for idx, inst in aws_instance.kafka_brokers : idx => inst.id }
  environment = var.environment
  instance_id = each.value
  az = element(data.aws_availability_zones.available.names, each.key % 3)
  kms_key_id = aws_kms_key.kafka.arn
}

module "ec2_broker" {
  source = "./modules/ec2-broker"
  environment = var.environment
  kafka_sg_id = module.vpc.kafka_sg_id
  private_subnet_ids = module.vpc.private_subnet_ids
  s3_bucket = module.s3_artifacts.bucket_name
  # ... other vars
  depends_on = [module.s3_artifacts]
}

module "mm2_cluster" {
  source = "./modules/mm2-cluster"
  environment = var.environment
  # ... similar
}

# IAM Role for EC2
resource "aws_iam_role" "kafka_role" {
  name = "${var.environment}-kafka-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Action = "sts:AssumeRole"
      Effect = "Allow"
      Principal = { Service = "ec2.amazonaws.com" }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "s3_policy" {
  role       = aws_iam_role.kafka_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
}

# KMS for EBS/SSL
resource "aws_kms_key" "kafka" {
  description = "KMS for Kafka EBS and SSL"
}
```

### outputs.tf
```hcl
output "kafka_bootstrap_servers" {
  value = "kafka-nlb.${var.environment}.elb.us-east-1.amazonaws.com:9092"
  description = "Bootstrap servers for clients"
}

output "mm2_status" {
  value = module.mm2_cluster.health_check_url
}
```

### configs/install/install-kafka.sh (Separate Script, Uploaded to S3 or Embedded)
```bash
#!/bin/bash
# Format and mount data dirs if not done
for dir in /data/kafka*; do
  if ! mountpoint -q "$dir"; then
    mkfs.xfs "${dir/dev}"  # Pseudo
    mount "${dir/dev}" "$dir"
  fi
  chown -R kafka:kafka "$dir"
done

# Format /app if needed (already in user-data)

# Systemd service
cat > /etc/systemd/system/kafka.service << 'EOF'
[Unit]
Description=Apache Kafka
After=network.target

[Service]
Type=simple
User=kafka
ExecStart=/app/kafka/bin/kafka-server-start.sh /etc/kafka/server.properties
ExecStop=/app/kafka/bin/kafka-server-stop.sh
Restart=always

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable kafka
```

Similar scripts for `install-java.sh` (set JAVA_HOME, PATH) and `install-python.sh` (pip install kafka-python, etc.).

For MM2: `install-mirrormaker.sh` with `kafka-mirror-maker.sh --config /etc/kafka/mm2.properties`.

## Deployment Instructions (README Excerpt)
1. **Init**: `terraform init -backend-config="bucket=terraform-state-${environment}"` (per env).
2. **Workspace**: `terraform workspace new ${environment}` or use `-var-file=environments/${environment}.tfvars`.
3. **Plan**: `terraform plan -var-file=environments/${environment}.tfvars`.
4. **Apply**: `terraform apply -var-file=environments/${environment}.tfvars`.
5. **Upload Artifacts**: Before apply, `aws s3 sync local-artifacts/ s3://kafka-artifacts-${environment}/`.
6. **Generate SSL/JAAS**: Use scripts like:
   ```bash
   keytool -keystore kafka.server.keystore.jks -alias kafka -validity 365 -genkey -keyalg RSA -storepass changeit
   # Distribute to S3
   ```
7. **Destroy**: `terraform destroy -var-file=...`.

## Test Plan (test-plan.md)
### Unit Tests (Terraform)
- Use `terraform validate` and `tflint`.
- Terratest: Go scripts to validate resources post-apply.
  Example:
  ```go
  package test

  import (
    "testing"
    "github.com/gruntwork-io/terratest/modules/terraform"
    "github.com/stretchr/testify/assert"
  )

  func TestKafkaCluster(t *testing.T) {
    terraformOptions := &terraform.Options{
      TerraformDir: "../",
      Vars:         map[string]interface{}{"environment": "dev"},
    }
    defer terraform.Destroy(t, terraformOptions)
    terraform.InitAndApply(t, terraformOptions)

    // Assert 12 instances
    instances := terraform.OutputList(t, terraformOptions, "ec2_instance_ids")
    assert.Equal(t, 12, len(instances))

    // Check security groups
    sg := terraform.Output(t, terraformOptions, "kafka_sg_id")
    // Validate ports via AWS SDK
  }
  ```

### Integration Tests
1. **Bootstrap Verification**:
   - SSH to a broker: `ssh -i key.pem ec2-user@ip`.
   - Check installs: `java -version`, `python --version`, `ls /app/kafka`.
   - Verify mounts: `df -h | grep data`.

2. **Kafka Functionality (KRaft)**:
   - Use Kafka client (from S3 or local): `kafka-topics --bootstrap-server ${output.kafka_bootstrap_servers} --create --topic test --partitions 3 --replication-factor 3 --command-config client.properties`.
   - Produce/Consume: Ensure SASL/SSL works (no auth errors).
   - Controller health: `kafka-controller-listener --bootstrap-server ... --command-config client.properties`.
   - Quorum check: Logs should show controllers elected.

3. **Storage Tests**:
   - Write to /app: Create file, verify persistence.
   - Kafka data: Produce messages, check log.dirs usage via `du -sh /data/kafka*`.

4. **MirrorMaker Tests**:
   - Create source topic, replicate to target.
   - Verify lag: `kafka-mirror-maker --consumer.config ... --describe`.

5. **Security Tests**:
   - Attempt unauth connection: Should fail.
   - Network: `nc -zv broker-ip 9092` from VPC only.
   - SSL: `openssl s_client -connect nlb:9092 -tls1_2` (verify cert).

6. **Performance/Scalability**:
   - Load test: Use `kafka-producer-perf-test` with 1M messages.
   - Failover: Stop a controller, verify election (under 10s).
   - AZ Failure: Simulate AZ outage, check replication.

7. **Env-Specific**:
   - Dev/QA: Smoke tests only.
   - Staging/Prod: Full chaos engineering (e.g., Netflix Chaos Monkey integration).

### Monitoring Tests
- CloudWatch alarms: CPU >80%, EBS IOPS.
- Logs: `/var/log/kafka/server.log` for errors.

### Rollback/DR
- `terraform apply` with scaled-down vars.
- Backup: EBS snapshots via lifecycle policy.

## Additional Considerations
- **Cost Optimization**: Use Spot instances for non-prod; Savings Plans for prod.
- **CI/CD**: GitHub Actions with `terraform plan` on PRs.
- **Secrets Management**: Use AWS SSM for passwords (inject via user-data `aws ssm get-parameter`).
- **Updates**: For Kafka upgrades, update S3 artifacts and re-apply (rolling ASG).
- **Limitations**: No auto-healing for KRaft quorum loss; monitor closely.
- **Extensions**: Add Prometheus exporter module; integrate with MSK for hybrid if needed.

This setup is modular, scalable, and secure. For customizations, adjust vars/modules. Contact for clarifications!