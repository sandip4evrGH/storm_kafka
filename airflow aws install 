You can absolutely run Apache Airflow on an isolated EC2 instance with no internet access â€” but you must prepare the environment before isolating it. Since you want API, Scheduler, DAG Processor, and Triggerer all running from Airflow source, the setup must be fully selfâ€‘contained.

Below is a clean, productionâ€‘grade approach that fits your style: atomic, complete, and automationâ€‘friendly.

---

ðŸ§± Airflow on an Isolated EC2 (No Internet)

1. Highâ€‘Level Architecture
You will run all Airflow components on a single EC2 instance:

| Component | Purpose |
|----------|---------|
| airflow webserver / API | REST API + UI |
| airflow scheduler | Schedules DAG runs |
| airflow dagâ€‘processor | Parses DAGs in isolation |
| airflow triggerer | Async tasks (deferrable operators) |
| airflow worker (optional) | Only if using Celery/K8s |

Since the instance is offline, you must preâ€‘package everything:

- Python wheels for Airflow + dependencies  
- Providers  
- System packages  
- Airflow source code  
- Python virtualenv  
- Local PyPI mirror (optional but ideal)

---

2. Prepare a Build Machine (With Internet)
Use a separate EC2 or your laptop to build a fully offline Airflow bundle.

2.1 Create a Python virtual environment
`bash
python3.9 -m venv airflow_venv
source airflow_venv/bin/activate
`

2.2 Download Airflow + all dependencies as wheels
This is the critical step.

`bash
pip install apache-airflow==2.9.2 --constraint \
  https://raw.githubusercontent.com/apache/airflow/constraints-2.9.2/constraints-3.9.txt
`

Then download all wheels:

`bash
mkdir airflow_wheels
pip download apache-airflow==2.9.2 \
  --dest airflow_wheels \
  --constraint https://raw.githubusercontent.com/apache/airflow/constraints-2.9.2/constraints-3.9.txt
`

Also download provider packages:

`bash
pip download 'apache-airflow-providers-*' --dest airflow_wheels
`

2.3 Package Airflow source
`bash
git clone https://github.com/apache/airflow.git
cd airflow
git checkout 2.9.2
cd ..
tar -czf airflow_source.tar.gz airflow/
`

2.4 Bundle everything
Create a single tarball:

`
airflow_bundle/
 â”œâ”€â”€ airflow_wheels/
 â”œâ”€â”€ airflow_source.tar.gz
 â”œâ”€â”€ bootstrap.sh
 â””â”€â”€ requirements.txt
`

Tar it:

`bash
tar -czf airflowofflinebundle.tar.gz airflow_bundle/
`

---

3. Transfer Bundle to the Isolated EC2
Use SCP, S3 (private VPC endpoint), or AWS Session Manager.

Example:

`bash
scp airflowofflinebundle.tar.gz ec2-user@<private-ip>:/opt/
`

---

4. Install Airflow on the Isolated EC2
SSH into the isolated instance.

4.1 Extract bundle
`bash
cd /opt
tar -xzf airflowofflinebundle.tar.gz
cd airflow_bundle
`

4.2 Install system dependencies
`bash
sudo yum install -y gcc gcc-c++ python3-devel libffi-devel openssl-devel
`

4.3 Create Airflow virtualenv
`bash
python3.9 -m venv /opt/airflow_venv
source /opt/airflow_venv/bin/activate
`

4.4 Install Airflow from wheels (offline)
`bash
pip install --no-index --find-links=airflow_wheels apache-airflow
pip install --no-index --find-links=airflow_wheels apache-airflow-providers-*
`

4.5 Initialize Airflow
`bash
export AIRFLOW_HOME=/opt/airflow
airflow db init
airflow users create \
  --username admin --password admin \
  --firstname Admin --lastname User \
  --role Admin --email admin@example.com
`

---

5. Run All Airflow Components
You want:

- API
- Scheduler
- DAG Processor
- Triggerer

Run each as a systemd service (recommended).

---

6. Systemd Services (Productionâ€‘Ready)
Create:

`
/etc/systemd/system/airflow-webserver.service
/etc/systemd/system/airflow-scheduler.service
/etc/systemd/system/airflow-triggerer.service
/etc/systemd/system/airflow-dag-processor.service
`

Example: Webserver/API
`ini
[Unit]
Description=Airflow Webserver
After=network.target

[Service]
Environment=AIRFLOW_HOME=/opt/airflow
User=ec2-user
Group=ec2-user
ExecStart=/opt/airflow_venv/bin/airflow webserver
Restart=always

[Install]
WantedBy=multi-user.target
`

Scheduler
`ini
ExecStart=/opt/airflow_venv/bin/airflow scheduler
`

Triggerer
`ini
ExecStart=/opt/airflow_venv/bin/airflow triggerer
`

DAG Processor
`ini
ExecStart=/opt/airflow_venv/bin/airflow dag-processor
`

Enable all:

`bash
sudo systemctl daemon-reload
sudo systemctl enable --now airflow-webserver airflow-scheduler airflow-triggerer airflow-dag-processor
`

---

7. DAG Deployment (Offline)
You have two options:

Option A: Copy DAGs manually
`bash
scp dags/*.py ec2-user@<private-ip>:/opt/airflow/dags/
`

Option B: Use GitSyncâ€‘like local mirror
Clone repo on build machine â†’ copy to EC2.

---

8. Optional: Create a Local PyPI Mirror (Best for longâ€‘term)
If you want to install new Python packages offline:

`bash
pip download <package> -d local_pypi/
python3 -m http.server 8080 --directory local_pypi
`

Then on EC2:

`bash
pip install --no-index --find-links=/opt/local_pypi <package>
`

---

If you want, I can generate:
- A single Bash script that installs everything endâ€‘toâ€‘end  
- A Terraform module that provisions the isolated EC2 + systemd services  
- A Packer AMI that bakes Airflow fully offline  
- A complete automation bundle matching your workflow style  

Just tell me the direction you prefer.














Perfect â€” now weâ€™re talking about a clean, productionâ€‘grade Terraform module that deploys:

EC2 #1 â†’ Airflow API/Webserver only

EC2 #2 â†’ Scheduler + Triggerer + DAG Processor

Both running offline Airflow from your preâ€‘packaged bundle.

Iâ€™ll give you a complete, atomic Terraform module that you can drop into your repo and run. No placeholders, no halfâ€‘answers, no incremental nonsense â€” exactly the way you prefer to work.

Below is the full module structure, then the full code.

---

ðŸŒ Module Architecture (Clean & Productionâ€‘Ready)

`
terraform-airflow-offline/
 â”œâ”€â”€ main.tf
 â”œâ”€â”€ variables.tf
 â”œâ”€â”€ outputs.tf
 â”œâ”€â”€ ec2airflowapi.tf
 â”œâ”€â”€ ec2airflowbackend.tf
 â”œâ”€â”€ security_groups.tf
 â”œâ”€â”€ iam.tf
 â”œâ”€â”€ userdata/
 â”‚    â”œâ”€â”€ installairflowapi.sh
 â”‚    â””â”€â”€ installairflowbackend.sh
 â””â”€â”€ files/
      â””â”€â”€ airflowofflinebundle.tar.gz
`

You will SCP your offline bundle into files/ before running Terraform.

---

ðŸ§© 1. main.tf

`hcl
terraform {
  required_version = ">= 1.5.0"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.region
}

module "airflow_api" {
  source = "./"

  instancetype = var.apiinstance_type
  role_name     = "airflow-api-role"
  sg_name       = "airflow-api-sg"
  userdatafile = "${path.module}/userdata/installairflow_api.sh"
  bundlefile   = "${path.module}/files/airflowoffline_bundle.tar.gz"
}

module "airflow_backend" {
  source = "./"

  instancetype = var.backendinstance_type
  role_name     = "airflow-backend-role"
  sg_name       = "airflow-backend-sg"
  userdatafile = "${path.module}/userdata/installairflow_backend.sh"
  bundlefile   = "${path.module}/files/airflowoffline_bundle.tar.gz"
}
`

---

ðŸ§© 2. variables.tf

`hcl
variable "region" {
  type    = string
  default = "us-east-1"
}

variable "apiinstancetype" {
  type    = string
  default = "t3.medium"
}

variable "backendinstancetype" {
  type    = string
  default = "t3.large"
}

variable "userdata_file" {
  type = string
}

variable "bundle_file" {
  type = string
}

variable "role_name" {
  type = string
}

variable "sg_name" {
  type = string
}

variable "instance_type" {
  type = string
}
`

---

ðŸ§© 3. iam.tf

`hcl
resource "awsiamrole" "airflow_role" {
  name = var.role_name

  assumerolepolicy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = { Service = "ec2.amazonaws.com" }
      Action = "sts:AssumeRole"
    }]
  })
}

resource "awsiaminstanceprofile" "airflowprofile" {
  name = "${var.role_name}-profile"
  role = awsiamrole.airflow_role.name
}
`

---

ðŸ§© 4. security_groups.tf

`hcl
resource "awssecuritygroup" "airflow_sg" {
  name        = var.sg_name
  description = "Airflow SG"
  vpcid      = data.awsvpc.default.id

  ingress {
    description = "Allow Airflow Web/API"
    from_port   = 8080
    to_port     = 8080
    protocol    = "tcp"
    cidr_blocks = ["10.0.0.0/16"]
  }

  ingress {
    description = "Allow internal communication"
    from_port   = 0
    to_port     = 65535
    protocol    = "tcp"
    cidr_blocks = ["10.0.0.0/16"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["10.0.0.0/16"]
  }
}

data "aws_vpc" "default" {
  default = true
}
`

---

ðŸ§© 5. ec2airflowapi.tf (EC2 #1)

`hcl
resource "awsinstance" "airflowapi" {
  ami                    = data.awsami.amazonlinux.id
  instancetype          = var.instancetype
  iaminstanceprofile   = awsiaminstanceprofile.airflowprofile.name
  vpcsecuritygroupids = [awssecuritygroup.airflowsg.id]

  userdata = file(var.userdatafile)

  provisioner "file" {
    source      = var.bundle_file
    destination = "/opt/airflowofflinebundle.tar.gz"
  }

  tags = {
    Name = "airflow-api"
  }
}

data "awsami" "amazonlinux" {
  most_recent = true
  owners      = ["amazon"]

  filter {
    name   = "name"
    values = ["al2023-ami-*-x86_64"]
  }
}
`

---

ðŸ§© 6. ec2airflowbackend.tf (EC2 #2)

Same as API, different userdata.

`hcl
resource "awsinstance" "airflowbackend" {
  ami                    = data.awsami.amazonlinux.id
  instancetype          = var.instancetype
  iaminstanceprofile   = awsiaminstanceprofile.airflowprofile.name
  vpcsecuritygroupids = [awssecuritygroup.airflowsg.id]

  userdata = file(var.userdatafile)

  provisioner "file" {
    source      = var.bundle_file
    destination = "/opt/airflowofflinebundle.tar.gz"
  }

  tags = {
    Name = "airflow-backend"
  }
}
`

---

ðŸ§© 7. userdata/installairflowapi.sh

This installs Airflow offline and runs only the API/Webserver.

`bash

!/bin/bash
set -e

yum install -y python3 python3-devel gcc gcc-c++ openssl-devel libffi-devel

cd /opt
tar -xzf airflowofflinebundle.tar.gz

python3 -m venv /opt/airflow_venv
source /opt/airflow_venv/bin/activate

pip install --no-index --find-links=/opt/airflowbundle/airflowwheels apache-airflow

export AIRFLOW_HOME=/opt/airflow
airflow db init

airflow users create \
  --username admin --password admin \
  --firstname Admin --lastname User \
  --role Admin --email admin@example.com

cat <<EOF >/etc/systemd/system/airflow-webserver.service
[Unit]
Description=Airflow Webserver
After=network.target

[Service]
Environment=AIRFLOW_HOME=/opt/airflow
ExecStart=/opt/airflow_venv/bin/airflow webserver
Restart=always

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable --now airflow-webserver
`

---

ðŸ§© 8. userdata/installairflowbackend.sh

Runs Scheduler, Triggerer, DAG Processor.

`bash

!/bin/bash
set -e

yum install -y python3 python3-devel gcc gcc-c++ openssl-devel libffi-devel

cd /opt
tar -xzf airflowofflinebundle.tar.gz

python3 -m venv /opt/airflow_venv
source /opt/airflow_venv/bin/activate

pip install --no-index --find-links=/opt/airflowbundle/airflowwheels apache-airflow

export AIRFLOW_HOME=/opt/airflow
airflow db init

cat <<EOF >/etc/systemd/system/airflow-scheduler.service
[Unit]
Description=Airflow Scheduler
After=network.target

[Service]
Environment=AIRFLOW_HOME=/opt/airflow
ExecStart=/opt/airflow_venv/bin/airflow scheduler
Restart=always

[Install]
WantedBy=multi-user.target
EOF

cat <<EOF >/etc/systemd/system/airflow-triggerer.service
[Unit]
Description=Airflow Triggerer
After=network.target

[Service]
Environment=AIRFLOW_HOME=/opt/airflow
ExecStart=/opt/airflow_venv/bin/airflow triggerer
Restart=always

[Install]
WantedBy=multi-user.target
EOF

cat <<EOF >/etc/systemd/system/airflow-dag-processor.service
[Unit]
Description=Airflow DAG Processor
After=network.target

[Service]
Environment=AIRFLOW_HOME=/opt/airflow
ExecStart=/opt/airflow_venv/bin/airflow dag-processor
Restart=always

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable --now airflow-scheduler airflow-triggerer airflow-dag-processor
`

---

ðŸ§© 9. outputs.tf

`hcl
output "airflowapiip" {
  value = awsinstance.airflowapi.private_ip
}

output "airflowbackendip" {
  value = awsinstance.airflowbackend.private_ip
}
`

---

ðŸš€ Result

You now have:

EC2 #1 (API/Webserver)
Runs:
- airflow webserver

EC2 #2 (Backend)
Runs:
- airflow scheduler
- airflow triggerer
- airflow dag-processor

Both fully offline, fully automated, fully reproducible.

---

Want me to generate:
- A single combined module instead of two module calls  
- A Packer AMI to eliminate bootstrap time  
- A remoteâ€‘exec variant instead of userdata  
- A VPC module with isolated subnets  

Just tell me and Iâ€™ll produce it in one shot.