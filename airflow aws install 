You can absolutely run Apache Airflow on an isolated EC2 instance with no internet access â€” but you must prepare the environment before isolating it. Since you want API, Scheduler, DAG Processor, and Triggerer all running from Airflow source, the setup must be fully selfâ€‘contained.

Below is a clean, productionâ€‘grade approach that fits your style: atomic, complete, and automationâ€‘friendly.

---

ðŸ§± Airflow on an Isolated EC2 (No Internet)

1. Highâ€‘Level Architecture
You will run all Airflow components on a single EC2 instance:

| Component | Purpose |
|----------|---------|
| airflow webserver / API | REST API + UI |
| airflow scheduler | Schedules DAG runs |
| airflow dagâ€‘processor | Parses DAGs in isolation |
| airflow triggerer | Async tasks (deferrable operators) |
| airflow worker (optional) | Only if using Celery/K8s |

Since the instance is offline, you must preâ€‘package everything:

- Python wheels for Airflow + dependencies  
- Providers  
- System packages  
- Airflow source code  
- Python virtualenv  
- Local PyPI mirror (optional but ideal)

---

2. Prepare a Build Machine (With Internet)
Use a separate EC2 or your laptop to build a fully offline Airflow bundle.

2.1 Create a Python virtual environment
`bash
python3.9 -m venv airflow_venv
source airflow_venv/bin/activate
`

2.2 Download Airflow + all dependencies as wheels
This is the critical step.

`bash
pip install apache-airflow==2.9.2 --constraint \
  https://raw.githubusercontent.com/apache/airflow/constraints-2.9.2/constraints-3.9.txt
`

Then download all wheels:

`bash
mkdir airflow_wheels
pip download apache-airflow==2.9.2 \
  --dest airflow_wheels \
  --constraint https://raw.githubusercontent.com/apache/airflow/constraints-2.9.2/constraints-3.9.txt
`

Also download provider packages:

`bash
pip download 'apache-airflow-providers-*' --dest airflow_wheels
`

2.3 Package Airflow source
`bash
git clone https://github.com/apache/airflow.git
cd airflow
git checkout 2.9.2
cd ..
tar -czf airflow_source.tar.gz airflow/
`

2.4 Bundle everything
Create a single tarball:

`
airflow_bundle/
 â”œâ”€â”€ airflow_wheels/
 â”œâ”€â”€ airflow_source.tar.gz
 â”œâ”€â”€ bootstrap.sh
 â””â”€â”€ requirements.txt
`

Tar it:

`bash
tar -czf airflowofflinebundle.tar.gz airflow_bundle/
`

---

3. Transfer Bundle to the Isolated EC2
Use SCP, S3 (private VPC endpoint), or AWS Session Manager.

Example:

`bash
scp airflowofflinebundle.tar.gz ec2-user@<private-ip>:/opt/
`

---

4. Install Airflow on the Isolated EC2
SSH into the isolated instance.

4.1 Extract bundle
`bash
cd /opt
tar -xzf airflowofflinebundle.tar.gz
cd airflow_bundle
`

4.2 Install system dependencies
`bash
sudo yum install -y gcc gcc-c++ python3-devel libffi-devel openssl-devel
`

4.3 Create Airflow virtualenv
`bash
python3.9 -m venv /opt/airflow_venv
source /opt/airflow_venv/bin/activate
`

4.4 Install Airflow from wheels (offline)
`bash
pip install --no-index --find-links=airflow_wheels apache-airflow
pip install --no-index --find-links=airflow_wheels apache-airflow-providers-*
`

4.5 Initialize Airflow
`bash
export AIRFLOW_HOME=/opt/airflow
airflow db init
airflow users create \
  --username admin --password admin \
  --firstname Admin --lastname User \
  --role Admin --email admin@example.com
`

---

5. Run All Airflow Components
You want:

- API
- Scheduler
- DAG Processor
- Triggerer

Run each as a systemd service (recommended).

---

6. Systemd Services (Productionâ€‘Ready)
Create:

`
/etc/systemd/system/airflow-webserver.service
/etc/systemd/system/airflow-scheduler.service
/etc/systemd/system/airflow-triggerer.service
/etc/systemd/system/airflow-dag-processor.service
`

Example: Webserver/API
`ini
[Unit]
Description=Airflow Webserver
After=network.target

[Service]
Environment=AIRFLOW_HOME=/opt/airflow
User=ec2-user
Group=ec2-user
ExecStart=/opt/airflow_venv/bin/airflow webserver
Restart=always

[Install]
WantedBy=multi-user.target
`

Scheduler
`ini
ExecStart=/opt/airflow_venv/bin/airflow scheduler
`

Triggerer
`ini
ExecStart=/opt/airflow_venv/bin/airflow triggerer
`

DAG Processor
`ini
ExecStart=/opt/airflow_venv/bin/airflow dag-processor
`

Enable all:

`bash
sudo systemctl daemon-reload
sudo systemctl enable --now airflow-webserver airflow-scheduler airflow-triggerer airflow-dag-processor
`

---

7. DAG Deployment (Offline)
You have two options:

Option A: Copy DAGs manually
`bash
scp dags/*.py ec2-user@<private-ip>:/opt/airflow/dags/
`

Option B: Use GitSyncâ€‘like local mirror
Clone repo on build machine â†’ copy to EC2.

---

8. Optional: Create a Local PyPI Mirror (Best for longâ€‘term)
If you want to install new Python packages offline:

`bash
pip download <package> -d local_pypi/
python3 -m http.server 8080 --directory local_pypi
`

Then on EC2:

`bash
pip install --no-index --find-links=/opt/local_pypi <package>
`

---

If you want, I can generate:
- A single Bash script that installs everything endâ€‘toâ€‘end  
- A Terraform module that provisions the isolated EC2 + systemd services  
- A Packer AMI that bakes Airflow fully offline  
- A complete automation bundle matching your workflow style  

Just tell me the direction you prefer.