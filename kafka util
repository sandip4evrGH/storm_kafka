Okay, here are the contents for the three files (topic_manager.py, bootstrap_servers.txt, and test_topic_manager.py) as plain text so you can easily copy them.
File: topic_manager.py
import asyncio
from kafka.admin import KafkaAdminClient
from kafka.client_async import KafkaClient
from kafka.consumer.simple import SimpleConsumer
import re
import threading
import argparse
import sys

topics = []

def get_bootstrap_servers_from_file(file_path, env):
    """
    Reads the bootstrap servers for a given environment from a file.
    Assumes each line in the file has the format: ENV=broker1:port1,broker2:port2,...
    """
    try:
        with open(file_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line.startswith(f"{env}="):
                    return line.split("=", 1)[1]
    except FileNotFoundError:
        print(f"Error: Bootstrap servers file '{file_path}' not found.")
        sys.exit(1)
    return None

def fetch_topics(env, bootstrap_servers, date_filter=None, type_filter=None, topic_filter_list=None):
    try:
        if not bootstrap_servers:
            print(f"Warning: No bootstrap servers found for environment '{env}'. Skipping.")
            return

        admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)
        topic_list = admin_client.list_topics()
        for topic_name in topic_list:
            match = re.match(r"(\w+)\.(\w+)\.(\w+)\.([\w\d]+)\.(\d{8})", topic_name)
            if match:
                prefix, middle1, middle2, env_from_topic, topic_date = match.groups()
                if env_from_topic.lower() in env.lower(): # Basic check if topic env matches provided env
                    if date_filter and topic_date != date_filter:
                        continue
                    if type_filter and middle1.upper() != type_filter.upper():
                        continue
                    if topic_filter_list and not any(filter_topic.upper() in topic_name.upper() for filter_topic in topic_filter_list):
                        continue

                    topic_details = {
                        "prefix": prefix,
                        "middle1": middle1,
                        "middle2": middle2,
                        "env_from_topic": env_from_topic,
                        "date": topic_date,
                        "total_messages": 0
                    }
                    topics.append({"env": env, "topic_name": topic_name, "details": topic_details})
        admin_client.close()
    except Exception as e:
        print(f"Error fetching topics for {env}: {e}")

def get_offsets(env, bootstrap_servers, topic_name, topic_details):
    try:
        if not bootstrap_servers:
            print(f"Warning: No bootstrap servers found for environment '{env}'. Skipping offset retrieval for {topic_name}.")
            return

        client = KafkaClient(bootstrap_servers=bootstrap_servers)
        consumer = SimpleConsumer(client, None, topic_name, auto_commit=False, fetch_size_bytes=4096 * 100)
        partitions = consumer.get_partition_ids()
        total_messages = 0
        for partition in partitions:
            earliest_offset = consumer.get_earliest_offsets(topic_name, [partition])[partition]
            latest_offset = consumer.get_latest_offsets(topic_name, [partition])[partition]
            total_messages += latest_offset - earliest_offset
        topic_details['total_messages'] = total_messages
        client.close()
    except Exception as e:
        print(f"Error getting offsets for {env}-{topic_name}: {e}")

def main():
    parser = argparse.ArgumentParser(description="Kafka Count Utility Script")
    parser.add_argument("-e", "--env", required=True, help="Comma-separated environments (e.g., dev1,prod3)")
    parser.add_argument("-d", "--date", help="Date filter for topics (e.g., 20250513)")
    parser.add_argument("-t", "--type", help="Enrichment type filter for topics (e.g., E)")
    parser.add_argument("--topic-filter", help="Comma-separated topic filters (e.g., BOOKINGTOOL,ORDERS)")
    parser.add_argument("-f", "--file", default="bootstrap_servers.txt", help="File containing environment and bootstrap Server list")
    parser.add_argument("--get-offset", action="store_true", help="Fetch offsets for topics")

    args = parser.parse_args()

    target_envs = [env.strip() for env in args.env.split(',')]
    date_filter = args.date
    type_filter = args.type
    topic_filter_list = [topic.strip() for topic in args.topic_filter.split(',')] if args.topic_filter else None
    bootstrap_file = args.file
    fetch_offsets = args.get_offset

    threads = []
    for env in target_envs:
        bootstrap_servers = get_bootstrap_servers_from_file(bootstrap_file, env)
        thread = threading.Thread(target=fetch_topics, args=(env, bootstrap_servers, date_filter, type_filter, topic_filter_list))
        threads.append(thread)
        thread.start()

    for thread in threads:
        thread.join()

    print("List of topics:")
    for topic_info in topics:
        print(f"Env: {topic_info['env']}, Topic: {topic_info['topic_name']}, Details: {topic_info['details']}")

    if fetch_offsets:
        offset_threads = []
        for topic_info in topics:
            bootstrap_servers = get_bootstrap_servers_from_file(bootstrap_file, topic_info['env'])
            thread = threading.Thread(target=get_offsets, args=(topic_info['env'], bootstrap_servers, topic_info['topic_name'], topic_info['details']))
            offset_threads.append(thread)
            thread.start()

        for thread in offset_threads:
            thread.join()

        print("\nTopic details with total messages:")
        for topic_info in topics:
            if "prefix" in topic_info['details'] and "env_from_topic" in topic_info['details']:
                print(f"{topic_info['details']['prefix'].upper()}, {topic_info['details']['env_from_topic'].upper()}, {topic_info['topic_name']}, {topic_info['details']['total_messages']}")

if __name__ == "__main__":
    main()

File: bootstrap_servers.txt
dev1=your_dev1_kafka_brokers
prod3=losthost.prod3:9092,anotherhost.prod3:9092
staging2=your_staging2_kafka_brokers
prod88=your_prod88_kafka_brokers
# Add more environments and their broker lists here

File: test_topic_manager.py
import subprocess
import unittest
import os
import tempfile
import re
import json
from unittest.mock import patch, MagicMock

class TestKafkaTopicManagerWithMockData(unittest.TestCase):

    def setUp(self):
        # Create a temporary bootstrap servers file for testing
        self.temp_bootstrap_file = tempfile.NamedTemporaryFile(mode='w', delete=False)
        self.temp_bootstrap_content = """
mock_env=mock-broker:9092
"""
        self.temp_bootstrap_file.write(self.temp_bootstrap_content)
        self.temp_bootstrap_file.close()

        # Load the mock Kafka data from the JSON file
        try:
            with open("mock_kafka_data.json", "r") as f:
                self.mock_kafka_data = json.load(f)
            self.mock_topic_names = list(self.mock_kafka_data.keys())
        except FileNotFoundError:
            print("Error: mock_kafka_data.json not found. Please run mock_kafka_generator.py first.")
            sys.exit(1)

    def tearDown(self):
        # Clean up the temporary bootstrap servers file
        os.unlink(self.temp_bootstrap_file.name)

    def run_script(self, *args):
        """Helper function to run the topic_manager.py script with given arguments."""
        command = ["python", "topic_manager.py"] + list(args)
        process = subprocess.run(command, capture_output=True, text=True, check=False)
        return process.returncode, process.stdout, process.stderr

    @patch('kafka.admin.KafkaAdminClient.list_topics')
    @patch('kafka.client_async.KafkaClient')
    @patch('kafka.consumer.simple.SimpleConsumer')
    def test_list_topics_and_get_offsets_with_mock_data(self, MockSimpleConsumer, MockKafkaClient, mock_list_topics):
        # Mock list_topics to return the topics from our mock data
        mock_list_topics.return_value = self.mock_topic_names

        # Mock KafkaClient and SimpleConsumer
        mock_consumer_instance = MagicMock()
        MockSimpleConsumer.return_value = mock_consumer_instance

        # Simulate partition IDs and offsets for each topic
        topic_partition_info = {}
        for topic, partitions in self.mock_kafka_data.items():
            partition_ids = list(partitions.keys())
            mock_consumer_instance.get_partition_ids.return_value = partition_ids
            earliest_offsets = {p: 0 for p in partition_ids}  # Mock earliest offset as 0
            latest_offsets = partitions
            mock_consumer_instance.get_earliest_offsets.return_value = earliest_offsets
            mock_consumer_instance.get_latest_offsets.return_value = latest_offsets
            topic_partition_info[topic] = sum(latest_offsets.values())

        returncode, stdout, stderr = self.run_script("-e", "mock_env", "-f", self.temp_bootstrap_file.name, "--get-offset")
        self.assertEqual(returncode, 0)

        # Assert that the "List of topics:" section contains the expected topics
        self.assertIn("List of topics:", stdout)
        for topic in self.mock_topic_names:
            self.assertIn(f"Env: mock_env, Topic: {topic}", stdout)

        # Assert that the "Topic details with total messages:" section contains the correct counts
        self.assertIn("Topic details with total messages:", stdout)
        for topic, total_messages in topic_partition_info.items():
            self.assertIn(f"MOCK, MOCK, {topic}, {total_messages}", stdout)

if __name__ == '__main__':
    unittest.main()

File: mock_kafka_generator.py
import json
import random

def create_mock_kafka_data(num_topics=None):
    """
    Generates mock Kafka topic data with random partitions and offsets.

    Args:
        num_topics (int, optional): The maximum number of topics to generate.
                                     Defaults to a random number up to 10.

    Returns:
        dict: A dictionary representing the mock Kafka data, where keys are
              topic names and values are dictionaries of partition offsets.
              Example:
              {
                  "topic_a": {0: 55, 1: 88, 2: 12},
                  "topic_b": {0: 10, 1: 95, 2: 30, 3: 70},
                  ...
              }
    """
    if num_topics is None:
        num_topics = random.randint(1, 10)

    mock_data = {}
    for i in range(num_topics):
        topic_name = f"mock_topic_{chr(ord('a') + i)}"
        num_partitions = random.randint(10, 20)
        partitions_data = {}
        for partition in range(num_partitions):
            offset = random.randint(0, 100)
            partitions_data[partition] = offset
        mock_data[topic_name] = partitions_data
    return mock_data

def save_mock_kafka_data(data, filename="mock_kafka_data.json"):
    """
    Saves the mock Kafka data to a JSON file.

    Args:
        data (dict): The mock Kafka data dictionary.
        filename (str, optional): The name of the file to save to.
                                 Defaults to "mock_kafka_data.json".
    """
    with open(filename, 'w') as f:
        json.dump(data, f, indent=4)
    print(f"Mock Kafka data saved to '{filename}'")

if __name__ == "__main__":
    mock_kafka = create_mock_kafka_data()
    save_mock_kafka_data(mock_kafka)
    print("\nExample of generated mock data:")
    print(json.dumps(mock_kafka, indent=4))

You can copy and paste the content of each code block into a plain text file and then save them with the respective names (topic_manager.py, bootstrap_servers.txt, test_topic_manager.py, and mock_kafka_generator.py). Remember to run mock_kafka_generator.py first to create the mock_kafka_data.json file before running the tests.
