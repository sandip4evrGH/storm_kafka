I understand your concern about consolidating all the Terraform configuration into a single `main.tf` file and your interest in why this was done instead of maintaining a modular structure, as well as structuring the Terraform configuration files (`*.tf`) more effectively. The previous response consolidated everything into `main.tf` to provide a single, copyable text block that could be easily converted into a PDF, as per your request. However, a modular structure is generally preferred for maintainability, scalability, and clarity in production environments. Below, I’ll explain why the consolidation was done, the benefits of a modular structure, and how to organize the Terraform files into a modular structure while maintaining all the functionality (Kafka KRaft cluster, autoscaling, monitoring, alerting, and AWS Secrets Manager integration with SASL/SSL). I’ll also provide the modularized Terraform files, supporting scripts, and documentation, ensuring you can copy them into separate files and convert them to a PDF if needed.

### Why Everything Was Consolidated in `main.tf`
The consolidation into a single `main.tf` was done to meet your request for a single, copyable document that includes all configurations, scripts, and documentation for easy PDF creation. Here’s the reasoning:

1. **Single Copyable File**: You requested "all the information so I can copy PDF," which implied a need for a self-contained text block that could be copied into a text editor, saved as markdown, and converted to PDF using tools like Pandoc or a word processor. A single `main.tf` with embedded scripts and documentation simplifies this process.

2. **Simplified Presentation**: Combining everything into one file avoids the need to manage multiple files during copying, ensuring all components (Terraform, scripts, README, etc.) are in one place for PDF generation.

3. **Avoiding File Management Overhead**: In a non-modular structure, you don’t need to create a directory structure or manage multiple files manually when copying the content, which is useful for quick documentation or sharing.

However, a single `main.tf` is not ideal for real-world deployments due to:
- **Lack of Modularity**: It’s harder to maintain, reuse, or update specific components (e.g., Kafka, networking, monitoring).
- **Readability**: A large file becomes unwieldy and difficult to navigate.
- **Collaboration**: Modular files allow teams to work on different components independently.

### Benefits of a Modular Structure
A modular structure organizes Terraform configurations into separate files and directories, each responsible for a specific component (e.g., networking, Kafka, monitoring). The benefits include:

1. **Maintainability**: Each module focuses on one aspect, making updates easier (e.g., modifying monitoring without touching Kafka).
2. **Reusability**: Modules can be reused across environments (dev, prod) or projects.
3. **Clarity**: Smaller, focused files are easier to read and understand.
4. **Scalability**: Adding new components (e.g., a new service) is simpler by creating a new module.
5. **Collaboration**: Teams can work on separate modules without conflicts.

### Modular Structure
Below is the recommended modular structure for the Kafka KRaft cluster, with separate Terraform files for each component. This structure retains all functionality (12-node cluster, autoscaling, partition reassignment, monitoring, alerting, SASL/SSL with Secrets Manager) while being organized for maintainability. I’ll provide the content of each file, followed by supporting scripts, test plans, and README, all of which can be copied into separate files and converted to PDF.

#### Directory Structure
```
kafka-kraft-cluster/
├── main.tf
├── variables.tf
├── outputs.tf
├── modules/
│   ├── networking/
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   └── outputs.tf
│   ├── kafka/
│   │   ├── main.tf
│   │   ├── autoscaling.tf
│   │   ├── reassignment.tf
│   │   ├── monitoring.tf
│   │   ├── alerting.tf
│   │   ├── secrets.tf
│   │   ├── variables.tf
│   │   ├── outputs.tf
│   │   └── user_data.sh
│   ├── mirrormaker/
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   ├── outputs.tf
│   │   └── user_data.sh
│   ├── monitoring/
│   │   ├── main.tf
│   │   ├── variables.tf
│   │   ├── outputs.tf
│   │   └── user_data.sh
├── lambda/
│   ├── reassign_partitions.py
│   ├── alert_remediation.py
│   └── deploy_lambda.sh
├── scripts/
│   ├── generate_reassignment.sh
│   ├── execute_reassignment.sh
│   ├── verify_reassignment.sh
│   ├── configure_monitoring.sh
│   ├── push_metrics.sh
│   ├── test_security.sh
│   ├── test_alerting.sh
│   ├── test_monitoring.sh
│   ├── test_reassignment.sh
│   ├── test_connectivity.sh
│   ├── test_mirrormaker.sh
│   ├── validate_kafka.sh
├── properties/
│   ├── server.properties.tftpl
│   ├── controller.properties.tftpl
│   ├── mirrormaker.properties.tftpl
│   ├── monitoring/
│   │   ├── prometheus.yml
│   │   ├── cloudwatch-agent.json
│   │   └── burrow.toml
├── environments/
│   ├── dev/
│   │   └── terraform.tfvars
├── test/
│   ├── test_plan.md
├── README.md
```

### Terraform Files

#### Root `main.tf`
```hcl
provider "aws" {
  region = var.region
}

module "networking" {
  source      = "./modules/networking"
  environment = var.environment
  vpc_cidr    = var.vpc_cidr
  azs         = var.azs
}

module "kafka" {
  source                    = "./modules/kafka"
  environment               = var.environment
  vpc_id                    = module.networking.vpc_id
  subnet_ids                = module.networking.subnet_ids
  security_group_id         = module.networking.security_group_id
  s3_bucket                 = var.s3_bucket
  ami_id                    = var.ami_id
  instance_type             = var.instance_type
  key_name                  = var.key_name
  sasl_username             = var.sasl_username
  sasl_password             = var.sasl_password
  alert_email               = var.alert_email
  azs                       = var.azs
}

module "mirrormaker" {
  source            = "./modules/mirrormaker"
  environment       = var.environment
  subnet_ids        = module.networking.subnet_ids
  security_group_id = module.networking.security_group_id
  s3_bucket         = var.s3_bucket
  ami_id            = var.ami_id
  instance_type     = var.instance_type
  key_name          = var.key_name
  secret_id         = module.kafka.sasl_secret_id
}

module "monitoring" {
  source            = "./modules/monitoring"
  environment       = var.environment
  subnet_ids        = module.networking.subnet_ids
  security_group_id = module.networking.security_group_id
  s3_bucket         = var.s3_bucket
  ami_id            = var.ami_id
  instance_type     = var.monitoring_instance_type
  key_name          = var.key_name
  secret_id         = module.kafka.sasl_secret_id
  kafka_brokers     = module.kafka.broker_ips
  kafka_controllers = module.kafka.controller_ips
}
```

#### Root `variables.tf`
```hcl
variable "region" {
  description = "AWS region"
  type        = string
  default     = "us-east-1"
}

variable "environment" {
  description = "Deployment environment (dev, qa, staging, prod)"
  type        = string
  default     = "dev"
}

variable "vpc_cidr" {
  description = "VPC CIDR block"
  type        = string
  default     = "10.0.0.0/16"
}

variable "azs" {
  description = "Availability zones"
  type        = list(string)
  default     = ["us-east-1a", "us-east-1b", "us-east-1c"]
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "m5.xlarge"
}

variable "monitoring_instance_type" {
  description = "EC2 instance type for monitoring"
  type        = string
  default     = "m5.large"
}

variable "s3_bucket" {
  description = "S3 bucket for Kafka binaries and configs"
  type        = string
  default     = "my-kafka-bucket"
}

variable "ami_id" {
  description = "AMI ID for EC2 instances"
  type        = string
  default     = "ami-12345678" # Replace with actual AMI
}

variable "key_name" {
  description = "SSH key pair name"
  type        = string
  default     = "my-key-pair"
}

variable "sasl_username" {
  description = "SASL username for Kafka"
  type        = string
  default     = "kafka-admin"
}

variable "sasl_password" {
  description = "SASL password for Kafka"
  type        = string
  sensitive   = true
  default     = "secure-password" # Replace with actual password
}

variable "alert_email" {
  description = "Email for SNS notifications"
  type        = string
  default     = "admin@example.com"
}
```

#### Root `outputs.tf`
```hcl
output "kafka_broker_ips" {
  description = "Private IPs of Kafka brokers"
  value       = module.kafka.broker_ips
}

output "kafka_controller_ips" {
  description = "Private IPs of Kafka controllers"
  value       = module.kafka.controller_ips
}

output "mirrormaker_ips" {
  description = "Private IPs of MirrorMaker instances"
  value       = module.mirrormaker.mirrormaker_ips
}

output "monitoring_ip" {
  description = "Private IP of monitoring instance"
  value       = module.monitoring.monitoring_ip
}

output "sasl_secret_id" {
  description = "AWS Secrets Manager secret ID for SASL credentials"
  value       = module.kafka.sasl_secret_id
}
```

#### Networking Module (`modules/networking/main.tf`)
```hcl
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_support   = true
  enable_dns_hostnames = true
  tags = {
    Name = "${var.environment}-kafka-vpc"
  }
}

resource "aws_subnet" "subnets" {
  count             = length(var.azs)
  vpc_id            = aws_vpc.main.id
  cidr_block        = cidrsubnet(var.vpc_cidr, 8, count.index)
  availability_zone = var.azs[count.index]
  tags = {
    Name = "${var.environment}-subnet-${count.index}"
  }
}

resource "aws_security_group" "kafka_sg" {
  vpc_id = aws_vpc.main.id
  name   = "${var.environment}-kafka-sg"

  ingress {
    from_port   = 9092
    to_port     = 9092
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr]
  }

  ingress {
    from_port   = 9093
    to_port     = 9093
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"] # Restrict in production
  }

  ingress {
    from_port   = 3000
    to_port     = 3000
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"] # Restrict in production
  }

  ingress {
    from_port   = 7071
    to_port     = 7071
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr]
  }

  ingress {
    from_port   = 8000
    to_port     = 8000
    protocol    = "tcp"
    cidr_blocks = [var.vpc_cidr]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}
```

#### Networking Module (`modules/networking/variables.tf`)
```hcl
variable "environment" {
  description = "Deployment environment"
  type        = string
}

variable "vpc_cidr" {
  description = "VPC CIDR block"
  type        = string
}

variable "azs" {
  description = "Availability zones"
  type        = list(string)
}
```

#### Networking Module (`modules/networking/outputs.tf`)
```hcl
output "vpc_id" {
  description = "VPC ID"
  value       = aws_vpc.main.id
}

output "subnet_ids" {
  description = "Subnet IDs"
  value       = aws_subnet.subnets[*].id
}

output "security_group_id" {
  description = "Security group ID"
  value       = aws_security_group.kafka_sg.id
}
```

#### Kafka Module (`modules/kafka/main.tf`)
```hcl
resource "aws_iam_role" "kafka_role" {
  name = "${var.environment}-kafka-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{ Action = "sts:AssumeRole", Effect = "Allow", Principal = { Service = "ec2.amazonaws.com" } }]
  })
}

resource "aws_iam_role_policy" "kafka_policy" {
  name = "${var.environment}-kafka-policy"
  role = aws_iam_role.kafka_role.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      { Effect = "Allow", Action = ["secretsmanager:GetSecretValue"], Resource = aws_secretsmanager_secret.kafka_sasl_credentials.arn },
      { Effect = "Allow", Action = ["s3:GetObject", "s3:ListBucket"], Resource = ["arn:aws:s3:::${var.s3_bucket}", "arn:aws:s3:::${var.s3_bucket}/*"] }
    ]
  })
}

resource "aws_iam_instance_profile" "kafka_instance_profile" {
  name = "${var.environment}-kafka-instance-profile"
  role = aws_iam_role.kafka_role.name
}

resource "aws_launch_template" "kafka_broker" {
  name_prefix   = "${var.environment}-kafka-broker-"
  image_id      = var.ami_id
  instance_type = var.instance_type
  key_name      = var.key_name
  iam_instance_profile { name = aws_iam_instance_profile.kafka_instance_profile.name }
  user_data = base64encode(templatefile("${path.module}/user_data.sh", {
    node_id            = "auto"
    is_controller      = false
    s3_bucket          = var.s3_bucket
    environment        = var.environment
    prometheus_version = "2.45.0"
    jmx_exporter_version = "0.16.1"
    secret_id          = aws_secretsmanager_secret.kafka_sasl_credentials.id
  }))

  network_interfaces {
    associate_public_ip_address = false
    security_groups             = [var.security_group_id]
  }

  block_device_mappings {
    device_name = "/dev/xvda"
    ebs {
      volume_size = 50
      volume_type = "gp3"
    }
  }

  tag_specifications {
    resource_type = "instance"
    tags = { Name = "${var.environment}-kafka-broker", Role = "broker" }
  }
}

resource "aws_autoscaling_group" "kafka_broker_asg" {
  name                = "${var.environment}-kafka-broker-asg"
  min_size            = 9
  max_size            = 18
  desired_capacity    = 9
  vpc_zone_identifier = var.subnet_ids
  launch_template {
    id      = aws_launch_template.kafka_broker.id
    version = "$Latest"
  }
  tag {
    key                 = "Name"
    value               = "${var.environment}-kafka-broker"
    propagate_at_launch = true
  }
}

resource "aws_instance" "kafka_controller" {
  count         = 3
  ami           = var.ami_id
  instance_type = var.instance_type
  subnet_id     = element(var.subnet_ids, count.index % length(var.azs))
  security_groups = [var.security_group_id]
  key_name      = var.key_name
  iam_instance_profile { name = aws_iam_instance_profile.kafka_instance_profile.name }
  user_data = base64encode(templatefile("${path.module}/user_data.sh", {
    node_id            = count.index
    is_controller      = true
    s3_bucket          = var.s3_bucket
    environment        = var.environment
    prometheus_version = "2.45.0"
    jmx_exporter_version = "0.16.1"
    secret_id          = aws_secretsmanager_secret.kafka_sasl_credentials.id
  }))
  tags = { Name = "${var.environment}-kafka-controller-${count.index}", Role = "controller" }
}

resource "aws_ebs_volume" "local_disks" {
  count             = (9 + 3) * 6
  availability_zone = element(var.azs, floor(count.index / 6) % length(var.azs))
  size              = 1000
  type              = "gp3"
  tags = { Name = "${var.environment}-local-disk-${format("%02d", count.index % 6 + 1)}" }
}

resource "aws_volume_attachment" "ebs_att" {
  count       = 9 * 6
  device_name = "/dev/xvd${count.index % 6 + 1}"
  volume_id   = aws_ebs_volume.local_disks[count.index].id
  instance_id = aws_autoscaling_group.kafka_broker_asg.instances[floor(count.index / 6)].id
}

resource "aws_volume_attachment" "controller_ebs_att" {
  count       = 3 * 6
  device_name = "/dev/xvd${count.index % 6 + 1}"
  volume_id   = aws_ebs_volume.local_disks[count.index + 54].id
  instance_id = aws_instance.kafka_controller[floor(count.index / 6)].id
}

resource "aws_s3_object" "kafka_config" {
  bucket = var.s3_bucket
  key    = "${var.environment}/kafka/server.properties"
  content = <<-EOF
broker.id=0
num.partitions=3
default.replication.factor=3
log.retention.hours=168
log.dirs=/mnt/local_disk02,/mnt/local_disk03,/mnt/local_disk04,/mnt/local_disk05,/mnt/local_disk06
listeners=SASL_SSL://0.0.0.0:9092
advertised.listeners=SASL_SSL://\${hostname}:9092
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
EOF
}

resource "aws_s3_object" "controller_config" {
  bucket = var.s3_bucket
  key    = "${var.environment}/kafka/controller.properties"
  content = <<-EOF
node.id=0
process.roles=controller
listeners=SASL_SSL://0.0.0.0:9093
advertised.listeners=SASL_SSL://\${hostname}:9093
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
EOF
}
```

#### Kafka Module (`modules/kafka/autoscaling.tf`)
```hcl
resource "aws_autoscaling_policy" "kafka_broker_scale_up" {
  name                   = "${var.environment}-kafka-broker-scale-up"
  autoscaling_group_name = aws_autoscaling_group.kafka_broker_asg.name
  adjustment_type        = "ChangeInCapacity"
  scaling_adjustment    = 1
  cooldown              = 300
}

resource "aws_autoscaling_policy" "kafka_broker_scale_down" {
  name                   = "${var.environment}-kafka-broker-scale-down"
  autoscaling_group_name = aws_autoscaling_group.kafka_broker_asg.name
  adjustment_type        = "ChangeInCapacity"
  scaling_adjustment    = -1
  cooldown              = 300
}

resource "aws_cloudwatch_metric_alarm" "kafka_broker_scale_up" {
  alarm_name          = "${var.environment}-kafka-broker-scale-up"
  comparison_operator = "GreaterThanOrEqualToThreshold"
  evaluation_periods  = 2
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = 300
  statistic           = "Average"
  threshold           = 70
  alarm_description   = "Scale up Kafka brokers when CPU utilization exceeds 70%"
  alarm_actions       = [aws_autoscaling_policy.kafka_broker_scale_up.arn]
  dimensions = { AutoScalingGroupName = aws_autoscaling_group.kafka_broker_asg.name }
}

resource "aws_cloudwatch_metric_alarm" "kafka_broker_scale_down" {
  alarm_name          = "${var.environment}-kafka-broker-scale-down"
  comparison_operator = "LessThanOrEqualToThreshold"
  evaluation_periods  = 2
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = 300
  statistic           = "Average"
  threshold           = 30
  alarm_description   = "Scale down Kafka brokers when CPU utilization drops below 30%"
  alarm_actions       = [aws_autoscaling_policy.kafka_broker_scale_down.arn]
  dimensions = { AutoScalingGroupName = aws_autoscaling_group.kafka_broker_asg.name }
}

resource "aws_cloudwatch_metric_alarm" "kafka_broker_lag_scale_up" {
  alarm_name          = "${var.environment}-kafka-broker-lag-scale-up"
  comparison_operator = "GreaterThanOrEqualToThreshold"
  evaluation_periods  = 2
  metric_name         = "ConsumerLag"
  namespace           = "Kafka"
  period              = 300
  statistic           = "Average"
  threshold           = 10000
  alarm_description   = "Scale up Kafka brokers when consumer lag exceeds 10,000"
  alarm_actions       = [aws_autoscaling_policy.kafka_broker_scale_up.arn]
  dimensions = { Environment = var.environment }
}
```

#### Kafka Module (`modules/kafka/reassignment.tf`)
```hcl
resource "aws_iam_role" "lambda_reassign_role" {
  name = "${var.environment}-kafka-reassign-lambda-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{ Action = "sts:AssumeRole", Effect = "Allow", Principal = { Service = "lambda.amazonaws.com" } }]
  })
}

resource "aws_iam_role_policy" "lambda_reassign_policy" {
  name = "${var.environment}-lambda-reassign-policy"
  role = aws_iam_role.lambda_reassign_role.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = ["autoscaling:DescribeAutoScalingGroups", "ec2:DescribeInstances", "cloudwatch:PutMetricData", "logs:*", "ssm:SendCommand", "secretsmanager:GetSecretValue"]
        Resource = ["*", aws_secretsmanager_secret.kafka_sasl_credentials.arn]
      }
    ]
  })
}

resource "aws_lambda_function" "reassign_partitions" {
  function_name = "${var.environment}-kafka-reassign-partitions"
  handler       = "reassign_partitions.lambda_handler"
  runtime       = "python3.9"
  role          = aws_iam_role.lambda_reassign_role.arn
  filename      = "${path.module}/../../lambda/reassign_partitions.zip"
  timeout       = 300
  environment {
    variables = {
      KAFKA_BOOTSTRAP = join(",", [for i in range(3) : "${aws_instance.kafka_controller[i].private_ip}:9092"])
      S3_BUCKET       = var.s3_bucket
      ENVIRONMENT     = var.environment
      SECRET_ID       = aws_secretsmanager_secret.kafka_sasl_credentials.id
    }
  }
}

resource "aws_cloudwatch_event_rule" "asg_event" {
  name        = "${var.environment}-kafka-asg-event"
  description = "Trigger partition reassignment on ASG events"
  event_pattern = jsonencode({
    source = ["aws.autoscaling"]
    detail-type = ["EC2 Instance Launch Successful", "EC2 Instance Terminate Successful"]
    detail = { AutoScalingGroupName = [aws_autoscaling_group.kafka_broker_asg.name] }
  })
}

resource "aws_cloudwatch_event_target" "lambda_target" {
  rule      = aws_cloudwatch_event_rule.asg_event.name
  target_id = "ReassignPartitions"
  arn       = aws_lambda_function.reassign_partitions.arn
}

resource "aws_lambda_permission" "allow_cloudwatch" {
  statement_id  = "AllowExecutionFromCloudWatch"
  action        = "lambda:InvokeFunction"
  function_name = aws_lambda_function.reassign_partitions.function_name
  principal     = "events.amazonaws.com"
  source_arn    = aws_cloudwatch_event_rule.asg_event.arn
}
```

#### Kafka Module (`modules/kafka/monitoring.tf`)
```hcl
resource "aws_s3_object" "prometheus_config" {
  bucket = var.s3_bucket
  key    = "${var.environment}/monitoring/prometheus.yml"
  content = <<-EOF
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: 'kafka'
    static_configs:
      - targets: [${join(",", [for instance in aws_autoscaling_group.kafka_broker_asg.instances : "\"${instance.private_ip}:7071\""])}]
    metrics_path: /metrics
EOF
}

resource "aws_s3_object" "cloudwatch_agent_config" {
  bucket = var.s3_bucket
  key    = "${var.environment}/monitoring/cloudwatch-agent.json"
  content = <<-EOF
{
  "metrics": {
    "namespace": "Kafka",
    "metrics_collected": [
      { "metric_name": "ConsumerLag", "metrics_path": "/mnt/local_disk01/push_metrics.sh" },
      { "metric_name": "BrokerRequestTime", "metrics_path": "/mnt/local_disk01/kafka/jmx-exporter.jar" }
    ]
  }
}
EOF
}

resource "aws_cloudwatch_log_group" "kafka_metrics" {
  name              = "/kafka/${var.environment}/metrics"
  retention_in_days = 7
}
```

#### Kafka Module (`modules/kafka/alerting.tf`)
```hcl
resource "aws_sns_topic" "kafka_alerts" {
  name = "${var.environment}-kafka-alerts"
}

resource "aws_sns_topic_subscription" "email_subscription" {
  topic_arn = aws_sns_topic.kafka_alerts.arn
  protocol  = "email"
  endpoint  = var.alert_email
}

resource "aws_iam_role" "lambda_remediation_role" {
  name = "${var.environment}-kafka-remediation-lambda-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{ Action = "sts:AssumeRole", Effect = "Allow", Principal = { Service = "lambda.amazonaws.com" } }]
  })
}

resource "aws_iam_role_policy" "lambda_remediation_policy" {
  name = "${var.environment}-lambda-remediation-policy"
  role = aws_iam_role.lambda_remediation_role.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = ["ec2:RebootInstances", "ssm:SendCommand", "cloudwatch:PutMetricData", "logs:*", "secretsmanager:GetSecretValue"]
        Resource = ["*", aws_secretsmanager_secret.kafka_sasl_credentials.arn]
      }
    ]
  })
}

resource "aws_lambda_function" "alert_remediation" {
  function_name = "${var.environment}-kafka-alert-remediation"
  handler       = "alert_remediation.lambda_handler"
  runtime       = "python3.9"
  role          = aws_iam_role.lambda_remediation_role.arn
  filename      = "${path.module}/../../lambda/alert_remediation.zip"
  timeout       = 300
  environment {
    variables = {
      KAFKA_BOOTSTRAP = join(",", [for i in range(3) : "${aws_instance.kafka_controller[i].private_ip}:9092"])
      ENVIRONMENT     = var.environment
      SECRET_ID       = aws_secretsmanager_secret.kafka_sasl_credentials.id
    }
  }
}

resource "aws_cloudwatch_metric_alarm" "consumer_lag" {
  alarm_name          = "${var.environment}-kafka-consumer-lag"
  comparison_operator = "GreaterThanOrEqualToThreshold"
  evaluation_periods  = 2
  metric_name         = "ConsumerLag"
  namespace           = "Kafka"
  period              = 300
  statistic           = "Average"
  threshold           = 10000
  alarm_description   = "Alert when consumer lag exceeds 10,000 messages"
  alarm_actions       = [aws_sns_topic.kafka_alerts.arn, aws_lambda_function.alert_remediation.arn]
  dimensions = { Environment = var.environment }
}

resource "aws_cloudwatch_metric_alarm" "broker_offline" {
  alarm_name          = "${var.environment}-kafka-broker-offline"
  comparison_operator = "LessThanThreshold"
  evaluation_periods  = 2
  metric_name         = "ActiveBrokerCount"
  namespace           = "Kafka"
  period              = 300
  statistic           = "Average"
  threshold           = 9
  alarm_description   = "Alert when fewer than 9 brokers are active"
  alarm_actions       = [aws_sns_topic.kafka_alerts.arn, aws_lambda_function.alert_remediation.arn]
  dimensions = { Environment = var.environment }
}

resource "aws_cloudwatch_metric_alarm" "under_replicated_partitions" {
  alarm_name          = "${var.environment}-kafka-under-replicated"
  comparison_operator = "GreaterThanOrEqualToThreshold"
  evaluation_periods  = 2
  metric_name         = "UnderReplicatedPartitions"
  namespace           = "Kafka"
  period              = 300
  statistic           = "Sum"
  threshold           = 1
  alarm_description   = "Alert when partitions are under-replicated"
  alarm_actions       = [aws_sns_topic.kafka_alerts.arn, aws_lambda_function.alert_remediation.arn]
  dimensions = { Environment = var.environment }
}

resource "aws_cloudwatch_metric_alarm" "cpu_utilization" {
  alarm_name          = "${var.environment}-kafka-cpu-high"
  comparison_operator = "GreaterThanOrEqualToThreshold"
  evaluation_periods  = 2
  metric_name         = "CPUUtilization"
  namespace           = "AWS/EC2"
  period              = 300
  statistic           = "Average"
  threshold           = 80
  alarm_description   = "Alert when CPU utilization exceeds 80%"
  alarm_actions       = [aws_sns_topic.kafka_alerts.arn, aws_autoscaling_policy.kafka_broker_scale_up.arn]
  dimensions = { AutoScalingGroupName = aws_autoscaling_group.kafka_broker_asg.name }
}

resource "aws_cloudwatch_metric_alarm" "disk_usage" {
  alarm_name          = "${var.environment}-kafka-disk-usage"
  comparison_operator = "GreaterThanOrEqualToThreshold"
  evaluation_periods  = 2
  metric_name         = "DiskUsedPercent"
  namespace           = "Kafka"
  period              = 300
  statistic           = "Average"
  threshold           = 90
  alarm_description   = "Alert when disk usage exceeds 90%"
  alarm_actions       = [aws_sns_topic.kafka_alerts.arn, aws_lambda_function.alert_remediation.arn]
  dimensions = { Environment = var.environment }
}
```

#### Kafka Module (`modules/kafka/secrets.tf`)
```hcl
resource "aws_secretsmanager_secret" "kafka_sasl_credentials" {
  name = "${var.environment}-kafka-sasl-credentials"
  description = "SASL credentials for Kafka cluster in ${var.environment}"
}

resource "aws_secretsmanager_secret_version" "kafka_sasl_credentials_version" {
  secret_id = aws_secretsmanager_secret.kafka_sasl_credentials.id
  secret_string = jsonencode({
    username = var.sasl_username
    password = var.sasl_password
  })
}
```

#### Kafka Module (`modules/kafka/variables.tf`)
```hcl
variable "environment" {
  description = "Deployment environment"
  type        = string
}

variable "vpc_id" {
  description = "VPC ID"
  type        = string
}

variable "subnet_ids" {
  description = "Subnet IDs"
  type        = list(string)
}

variable "security_group_id" {
  description = "Security group ID"
  type        = string
}

variable "s3_bucket" {
  description = "S3 bucket for Kafka binaries and configs"
  type        = string
}

variable "ami_id" {
  description = "AMI ID for EC2 instances"
  type        = string
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
}

variable "key_name" {
  description = "SSH key pair name"
  type        = string
}

variable "sasl_username" {
  description = "SASL username for Kafka"
  type        = string
}

variable "sasl_password" {
  description = "SASL password for Kafka"
  type        = string
  sensitive   = true
}

variable "alert_email" {
  description = "Email for SNS notifications"
  type        = string
}

variable "azs" {
  description = "Availability zones"
  type        = list(string)
}
```

#### Kafka Module (`modules/kafka/outputs.tf`)
```hcl
output "broker_ips" {
  description = "Private IPs of Kafka brokers"
  value       = aws_autoscaling_group.kafka_broker_asg.instances[*].private_ip
}

output "controller_ips" {
  description = "Private IPs of Kafka controllers"
  value       = aws_instance.kafka_controller[*].private_ip
}

output "sasl_secret_id" {
  description = "AWS Secrets Manager secret ID"
  value       = aws_secretsmanager_secret.kafka_sasl_credentials.id
}
```

#### Kafka Module (`modules/kafka/user_data.sh`)
```bash
#!/bin/bash
yum update -y
yum install -y java-11 python3 awscli jq
for i in {1..6}; do
  mkfs.ext4 /dev/xvd${i}
  mkdir -p /mnt/local_disk${i}
  mount /dev/xvd${i} /mnt/local_disk${i}
  echo "/dev/xvd${i} /mnt/local_disk${i} ext4 defaults,nofail 0 2" >> /etc/fstab
done
aws s3 cp s3://${s3_bucket}/kafka/kafka_2.13-3.9.0.tgz /mnt/local_disk01/
tar -xzf /mnt/local_disk01/kafka_2.13-3.9.0.tgz -C /mnt/local_disk01/
ln -s /mnt/local_disk01/kafka_2.13-3.9.0 /mnt/local_disk01/kafka
aws s3 cp s3://${s3_bucket}/${environment}/kafka/${is_controller ? "controller" : "server"}.properties /mnt/local_disk01/kafka/config/
USERNAME=$(aws secretsmanager get-secret-value --secret-id ${secret_id} --query SecretString --output text | jq -r '.username')
PASSWORD=$(aws secretsmanager get-secret-value --secret-id ${secret_id} --query SecretString --output text | jq -r '.password')
aws s3 cp s3://${s3_bucket}/kafka/ssl/kafka.truststore.jks /mnt/local_disk01/kafka/ssl/
aws s3 cp s3://${s3_bucket}/kafka/ssl/kafka.keystore.jks /mnt/local_disk01/kafka/ssl/
cat <<EOT > /mnt/local_disk01/kafka/config/ssl.properties
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="$USERNAME" password="$PASSWORD";
ssl.truststore.location=/mnt/local_disk01/kafka/ssl/kafka.truststore.jks
ssl.keystore.location=/mnt/local_disk01/kafka/ssl/kafka.keystore.jks
EOT
${is_controller ? "sed -i \"s/node.id=.*/node.id=${node_id}/\" /mnt/local_disk01/kafka/config/controller.properties" : "node_id=$(curl -s http://169.254.169.254/latest/meta-data/instance-id | cut -d'-' -f3); sed -i \"s/broker.id=.*/broker.id=$node_id/\" /mnt/local_disk01/kafka/config/server.properties"}
aws s3 cp s3://${s3_bucket}/monitoring/jmx-exporter-${jmx_exporter_version}.jar /mnt/local_disk01/kafka/jmx-exporter.jar
cat <<EOT > /mnt/local_disk01/kafka/config/jmx-exporter.yml
hostPort: 0.0.0.0:7071
rules:
  - pattern: "kafka.server<type=BrokerTopicMetrics, name=(.*)><>.*"
EOT
java -jar /mnt/local_disk01/kafka/jmx-exporter.jar 7071 /mnt/local_disk01/kafka/config/jmx-exporter.yml &
aws s3 cp s3://${s3_bucket}/monitoring/amazon-cloudwatch-agent.rpm /mnt/local_disk01/
yum install -y /mnt/local_disk01/amazon-cloudwatch-agent.rpm
aws s3 cp s3://${s3_bucket}/${environment}/monitoring/cloudwatch-agent.json /mnt/local_disk01/cloudwatch-agent.json
/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/mnt/local_disk01/cloudwatch-agent.json -s
cat <<EOT > /mnt/local_disk01/push_metrics.sh
#!/bin/bash
BROKER_COUNT=\$(/mnt/local_disk01/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server localhost:9092 --command-config /mnt/local_disk01/kafka/config/ssl.properties | grep -c broker)
aws cloudwatch put-metric-data --metric-name ActiveBrokerCount --namespace Kafka --value \$BROKER_COUNT --dimensions Environment=${environment}
UNDER_REPLICATED=\$(/mnt/local_disk01/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --under-replicated-partitions --command-config /mnt/local_disk01/kafka/config/ssl.properties | wc -l)
aws cloudwatch put-metric-data --metric-name UnderReplicatedPartitions --namespace Kafka --value \$UNDER_REPLICATED --dimensions Environment=${environment}
DISK_USED=\$(df -h /mnt/local_disk02 | tail -1 | awk '{print \$5}' | tr -d '%')
aws cloudwatch put-metric-data --metric-name DiskUsedPercent --namespace Kafka --value \$DISK_USED --dimensions Environment=${environment}
LAG=\$(/mnt/local_disk01/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group test-group --command-config /mnt/local_disk01/kafka/config/ssl.properties | grep -v LAG | awk '{sum+=\$6} END {print sum}')
aws cloudwatch put-metric-data --metric-name ConsumerLag --namespace Kafka --value \$LAG --dimensions Environment=${environment}
EOT
chmod +x /mnt/local_disk01/push_metrics.sh
(crontab -l; echo "* * * * * /mnt/local_disk01/push_metrics.sh") | crontab -
/mnt/local_disk01/kafka/bin/kafka-server-start.sh /mnt/local_disk01/kafka/config/${is_controller ? "controller" : "server"}.properties &
```

#### MirrorMaker Module (`modules/mirrormaker/main.tf`)
```hcl
resource "aws_iam_role" "mirrormaker_role" {
  name = "${var.environment}-mirrormaker-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{ Action = "sts:AssumeRole", Effect = "Allow", Principal = { Service = "ec2.amazonaws.com" } }]
  })
}

resource "aws_iam_role_policy" "mirrormaker_policy" {
  name = "${var.environment}-mirrormaker-policy"
  role = aws_iam_role.mirrormaker_role.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      { Effect = "Allow", Action = ["secretsmanager:GetSecretValue"], Resource = var.secret_id },
      { Effect = "Allow", Action = ["s3:GetObject", "s3:ListBucket"], Resource = ["arn:aws:active://:${var.s3_bucket}", "arn:aws:s3:::${var.s3_bucket}/*"] }
    ]
  })
}

resource "aws_iam_instance_profile" "mirrormaker_instance_profile" {
  name = "${var.environment}-mirrormaker-instance-profile"
  role = aws_iam_role.mirrormaker_role.name
}

resource "aws_instance" "mirrormaker" {
  count         = 4
  ami           = var.ami_id
  instance_type = var.instance_type
  subnet_id     = element(var.subnet_ids, count.index % length(var.subnet_ids))
  security_groups = [var.security_group_id]
  key_name      = var.key_name
  iam_instance_profile { name = aws_iam_instance_profile.mirrormaker_instance_profile.name }
  user_data = base64encode(templatefile("${path.module}/user_data.sh", {
    s3_bucket   = var.s3_bucket
    environment = var.environment
    secret_id   = var.secret_id
  }))
  tags = { Name = "${var.environment}-mirrormaker-${count.index}" }
}

resource "aws_s3_object" "mirrormaker_config" {
  bucket = var.s3_bucket
  key    = "${var.environment}/mirrormaker/mirrormaker.properties"
  content = <<-EOF
bootstrap.servers=source-cluster:9092
source.cluster.alias=source
destination.cluster.alias=dest
source.cluster.bootstrap.servers=source-kafka:9092
dest.cluster.bootstrap.servers=dest-kafka:9092
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
EOF
}
```

#### MirrorMaker Module (`modules/mirrormaker/variables.tf`)
```hcl
variable "environment" {
  description = "Deployment environment"
  type        = string
}

variable "subnet_ids" {
  description = "Subnet IDs"
  type        = list(string)
}

variable "security_group_id" {
  description = "Security group ID"
  type        = string
}

variable "s3_bucket" {
  description = "S3 bucket for Kafka binaries and configs"
  type        = string
}

variable "ami_id" {
  description = "AMI ID for EC2 instances"
  type        = string
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
}

variable "key_name" {
  description = "SSH key pair name"
  type        = string
}

variable "secret_id" {
  description = "AWS Secrets Manager secret ID"
  type        = string
}
```

#### MirrorMaker Module (`modules/mirrormaker/outputs.tf`)
```hcl
output "mirrormaker_ips" {
  description = "Private IPs of MirrorMaker instances"
  value       = aws_instance.mirrormaker[*].private_ip
}
```

#### MirrorMaker Module (`modules/mirrormaker/user_data.sh`)
```bash
#!/bin/bash
yum update -y
yum install -y java-11 python3 awscli
aws s3 cp s3://${s3_bucket}/kafka/kafka_2.13-3.9.0.tgz /opt/
tar -xzf /opt/kafka_2.13-3.9.0.tgz -C /opt/
ln -s /opt/kafka_2.13-3.9.0 /opt/kafka
aws s3 cp s3://${s3_bucket}/${environment}/mirrormaker/mirrormaker.properties /opt/kafka/config/
USERNAME=$(aws secretsmanager get-secret-value --secret-id ${secret_id} --query SecretString --output text | jq -r '.username')
PASSWORD=$(aws secretsmanager get-secret-value --secret-id ${secret_id} --query SecretString --output text | jq -r '.password')
aws s3 cp s3://${s3_bucket}/kafka/ssl/kafka.truststore.jks /opt/kafka/ssl/
aws s3 cp s3://${s3_bucket}/kafka/ssl/kafka.keystore.jks /opt/kafka/ssl/
cat <<EOT > /opt/kafka/config/mirrormaker-ssl.properties
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="$USERNAME" password="$PASSWORD";
ssl.truststore.location=/opt/kafka/ssl/kafka.truststore.jks
ssl.keystore.location=/opt/kafka/ssl/kafka.keystore.jks
EOT
sed -i "s|security.protocol=.*|security.protocol=SASL_SSL|" /opt/kafka/config/mirrormaker.properties
sed -i "s|sasl.mechanism=.*|sasl.mechanism=SCRAM-SHA-256|" /opt/kafka/config/mirrormaker.properties
/opt/kafka/bin/kafka-mirror-maker.sh --config /opt/kafka/config/mirrormaker.properties &
```

#### Monitoring Module (`modules/monitoring/main.tf`)
```hcl
resource "aws_iam_role" "monitoring_role" {
  name = "${var.environment}-monitoring-role"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{ Action = "sts:AssumeRole", Effect = "Allow", Principal = { Service = "ec2.amazonaws.com" } }]
  })
}

resource "aws_iam_role_policy" "monitoring_policy" {
  name = "${var.environment}-monitoring-policy"
  role = aws_iam_role.monitoring_role.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      { Effect = "Allow", Action = ["secretsmanager:GetSecretValue"], Resource = var.secret_id },
      { Effect = "Allow", Action = ["s3:GetObject", "s3:ListBucket"], Resource = ["arn:aws:s3:::${var.s3_bucket}", "arn:aws:s3:::${var.s3_bucket}/*"] }
    ]
  })
}

resource "aws_iam_instance_profile" "monitoring_instance_profile" {
  name = "${var.environment}-monitoring-instance-profile"
  role = aws_iam_role.monitoring_role.name
}

resource "aws_instance" "monitoring" {
  ami           = var.ami_id
  instance_type = var.instance_type
  subnet_id     = var.subnet_ids[0]
  security_groups = [var.security_group_id]
  key_name      = var.key_name
  iam_instance_profile { name = aws_iam_instance_profile.monitoring_instance_profile.name }
  user_data = base64encode(templatefile("${path.module}/user_data.sh", {
    s3_bucket        = var.s3_bucket
    environment      = var.environment
    grafana_version  = "9.5.3"
    burrow_version   = "1.6.0"
    secret_id        = var.secret_id
    kafka_controllers = var.kafka_controllers
  }))
  tags = { Name = "${var.environment}-monitoring" }
}

resource "aws_s3_object" "burrow_config" {
  bucket = var.s3_bucket
  key    = "${var.environment}/monitoring/burrow.toml"
  content = <<-EOF
[general]
loglevel="info"
[kafka]
client-id="burrow-client"
brokers=[${join(",", [for ip in var.kafka_controllers : "\"${ip}:9092\""])}]
sasl-mechanism="SCRAM-SHA-256"
security-protocol="SASL_SSL"
sasl-username="\${USERNAME}"
sasl-password="\${PASSWORD}"
EOF
}
```

#### Monitoring Module (`modules/monitoring/variables.tf`)
```hcl
variable "environment" {
  description = "Deployment environment"
  type        = string
}

variable "subnet_ids" {
  description = "Subnet IDs"
  type        = list(string)
}

variable "security_group_id" {
  description = "Security group ID"
  type        = string
}

variable "s3_bucket" {
  description = "S3 bucket for Kafka binaries and configs"
  type        = string
}

variable "ami_id" {
  description = "AMI ID for EC2 instances"
  type        = string
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
}

variable "key_name" {
  description = "SSH key pair name"
  type        = string
}

variable "secret_id" {
  description = "AWS Secrets Manager secret ID"
  type        = string
}

variable "kafka_brokers" {
  description = "Kafka broker IPs"
  type        = list(string)
}

variable "kafka_controllers" {
  description = "Kafka controller IPs"
  type        = list(string)
}
```

#### Monitoring Module (`modules/monitoring/outputs.tf`)
```hcl
output "monitoring_ip" {
  description = "Private IP of monitoring instance"
  value       = aws_instance.monitoring.private_ip
}
```

#### Monitoring Module (`modules/monitoring/user_data.sh`)
```bash
#!/bin/bash
yum update -y
yum install -y java-11 python3 awscli
aws s3 cp s3://${s3_bucket}/monitoring/grafana-${grafana_version}.rpm /opt/
yum install -y /opt/grafana-${grafana_version}.rpm
systemctl start grafana-server
systemctl enable grafana-server
aws s3 cp s3://${s3_bucket}/monitoring/burrow-${burrow_version}.tar.gz /opt/
tar -xzf /opt/burrow-${burrow_version}.tar.gz -C /opt/
USERNAME=$(aws secretsmanager get-secret-value --secret-id ${secret_id} --query SecretString --output text | jq -r '.username')
PASSWORD=$(aws secretsmanager get-secret-value --secret-id ${secret_id} --query SecretString --output text | jq -r '.password')
aws s3 cp s3://${s3_bucket}/${environment}/monitoring/burrow.toml /opt/burrow/config/burrow.toml
sed -i "s|sasl-username=\".*\"|sasl-username=\"$USERNAME\"|" /opt/burrow/config/burrow.toml
sed -i "s|sasl-password=\".*\"|sasl-password=\"$PASSWORD\"|" /opt/burrow/config/burrow.toml
/opt/burrow/burrow --config /opt/burrow/config/burrow.toml &
aws s3 cp s3://${s3_bucket}/monitoring/prometheus-2.45.0.tar.gz /opt/
tar -xzf /opt/prometheus-2.45.0.tar.gz -C /opt/
aws s3 cp s3://${s3_bucket}/${environment}/monitoring/prometheus.yml /opt/prometheus/prometheus.yml
/opt/prometheus/prometheus --config.file=/opt/prometheus/prometheus.yml &
```

### Lambda Functions

#### `lambda/reassign_partitions.py`
```python
import json
import boto3
import os
import subprocess

def lambda_handler(event, context):
    s3_bucket = os.environ['S3_BUCKET']
    environment = os.environ['ENVIRONMENT']
    bootstrap_servers = os.environ['KAFKA_BOOTSTRAP']
    secret_id = os.environ['SECRET_ID']
    
    ssm = boto3.client('ssm')
    s3 = boto3.client('s3')
    secretsmanager = boto3.client('secretsmanager')
    
    secret = secretsmanager.get_secret_value(SecretId=secret_id)
    username = json.loads(secret['SecretString'])['username']
    password = json.loads(secret['SecretString'])['password']
    
    with open('/tmp/ssl.properties', 'w') as f:
        f.write(f"""
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="{username}" password="{password}";
ssl.truststore.location=/mnt/local_disk01/kafka/ssl/kafka.truststore.jks
ssl.keystore.location=/mnt/local_disk01/kafka/ssl/kafka.keystore.jks
""")
    
    brokers = get_broker_list(bootstrap_servers)
    reassignment_file = f"/tmp/reassignment-{environment}.json"
    subprocess.run([
        "/mnt/local_disk01/kafka/bin/kafka-reassign-partitions.sh",
        "--bootstrap-server", bootstrap_servers,
        "--broker-list", ",".join(map(str, brokers)),
        "--topics-to-move-json-file", "/tmp/topics.json",
        "--generate",
        "--command-config", "/tmp/ssl.properties"
    ], capture_output=True, text=True)
    
    s3.upload_file(reassignment_file, s3_bucket, f"{environment}/reassignment/reassignment.json")
    
    response = ssm.send_command(
        InstanceIds=[get_random_broker_instance()],
        DocumentName="AWS-RunShellScript",
        Parameters={
            "commands": [
                f"aws s3 cp s3://{s3_bucket}/{environment}/reassignment/reassignment.json /mnt/local_disk01/reassignment.json",
                f"/mnt/local_disk01/kafka/bin/kafka-reassign-partitions.sh --bootstrap-server {bootstrap_servers} --reassignment-json-file /mnt/local_disk01/reassignment.json --execute --command-config /tmp/ssl.properties"
            ]
        }
    )
    
    return { 'statusCode': 200, 'body': json.dumps('Partition reassignment triggered') }

def get_broker_list(bootstrap_servers):
    result = subprocess.run([
        "/mnt/local_disk01/kafka/bin/kafka-broker-api-versions.sh",
        "--bootstrap-server", bootstrap_servers,
        "--command-config", "/tmp/ssl.properties"
    ], capture_output=True, text=True)
    return [int(line.split(":")[0]) for line in result.stdout.split("\n") if "broker" in line]

def get_random_broker_instance():
    ec2 = boto3.client('ec2')
    response = ec2.describe_instances(Filters=[{'Name': 'tag:Role', 'Values': ['broker']}])
    return response['Reservations'][0]['Instances'][0]['InstanceId']
```

#### `lambda/alert_remediation.py`
```python
import json
import boto3
import os
import subprocess

def lambda_handler(event, context):
    ssm = boto3.client('ssm')
    ec2 = boto3.client('ec2')
    environment = os.environ['ENVIRONMENT']
    bootstrap_servers = os.environ['KAFKA_BOOTSTRAP']
    secret_id = os.environ['SECRET_ID']
    
    secretsmanager = boto3.client('secretsmanager')
    secret = secretsmanager.get_secret_value(SecretId=secret_id)
    username = json.loads(secret['SecretString'])['username']
    password = json.loads(secret['SecretString'])['password']
    
    with open('/tmp/ssl.properties', 'w') as f:
        f.write(f"""
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="{username}" password="{password}";
ssl.truststore.location=/mnt/local_disk01/kafka/ssl/kafka.truststore.jks
ssl.keystore.location=/mnt/local_disk01/kafka/ssl/kafka.keystore.jks
""")
    
    alarm_name = event['detail']['AlarmName']
    
    if "broker-offline" in alarm_name:
        unhealthy_instances = get_unhealthy_brokers()
        for instance_id in unhealthy_instances:
            ec2.reboot_instances(InstanceIds=[instance_id])
    
    if "under-replicated" in alarm_name or "consumer-lag" in alarm_name:
        subprocess.run([
            "/mnt/local_disk01/kafka/bin/kafka-reassign-partitions.sh",
            "--bootstrap-server", bootstrap_servers,
            "--reassignment-json-file", "/mnt/local_disk01/reassignment.json",
            "--execute",
            "--command-config", "/tmp/ssl.properties"
        ], capture_output=True, text=True)
    
    if "disk-usage" in alarm_name:
        print("High disk usage detected, consider increasing EBS volume size")
    
    return { 'statusCode': 200, 'body': json.dumps(f'Remediation triggered for {alarm_name}') }

def get_unhealthy_brokers():
    ec2 = boto3.client('ec2')
    response = ec2.describe_instances(Filters=[{'Name': 'tag:Role', 'Values': ['broker']}, {'Name': 'instance-state-name', 'Values': ['running']}])
    return [instance['InstanceId'] for instance in response['Reservations'][0]['Instances']]
```

#### `lambda/deploy_lambda.sh`
```bash
#!/bin/bash
zip -r reassign_partitions.zip reassign_partitions.py
zip -r alert_remediation.zip alert_remediation.py
aws s3 cp reassign_partitions.zip s3://${s3_bucket}/lambda/reassign_partitions.zip
aws s3 cp alert_remediation.zip s3://${s3_bucket}/lambda/alert_remediation.zip
```

### Scripts

#### `scripts/generate_reassignment.sh`
```bash
#!/bin/bash
BOOTSTRAP_SERVERS=$1
ENVIRONMENT=$2
S3_BUCKET=$3
USERNAME=$(aws secretsmanager get-secret-value --secret-id ${ENVIRONMENT}-kafka-sasl-credentials --query SecretString --output text | jq -r '.username')
PASSWORD=$(aws secretsmanager get-secret-value --secret-id ${ENVIRONMENT}-kafka-sasl-credentials --query SecretString --output text | jq -r '.password')
cat <<EOT > /mnt/local_disk01/kafka/config/ssl.properties
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="$USERNAME" password="$PASSWORD";
ssl.truststore.location=/mnt/local_disk01/kafka/ssl/kafka.truststore.jks
ssl.keystore.location=/mnt/local_disk01/kafka/ssl/kafka.keystore.jks
EOT
/mnt/local_disk01/kafka/bin/kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS --list --command-config /mnt/local_disk01/kafka/config/ssl.properties > /tmp/topics.txt
echo '{"topics": [' > /tmp/topics.json
while read topic; do
  echo "{\"topic\": \"$topic\"}," >> /tmp/topics.json
done < /tmp/topics.txt
echo '], "version":1}' >> /tmp/topics.json
/mnt/local_disk01/kafka/bin/kafka-reassign-partitions.sh \
  --bootstrap-server $BOOTSTRAP_SERVERS \
  --topics-to-move-json-file /tmp/topics.json \
  --broker-list "$(/mnt/local_disk01/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server $BOOTSTRAP_SERVERS --command-config /mnt/local_disk01/kafka/config/ssl.properties | grep broker | cut -d':' -f1 | tr '\n' ',' | sed 's/,$//')" \
  --generate \
  --command-config /mnt/local_disk01/kafka/config/ssl.properties > /tmp/reassignment-$ENVIRONMENT.json
aws s3 cp /tmp/reassignment-$ENVIRONMENT.json s3://$S3_BUCKET/$ENVIRONMENT/reassignment/reassignment.json
```

#### `scripts/execute_reassignment.sh`
```bash
#!/bin/bash
BOOTSTRAP_SERVERS=$1
ENVIRONMENT=$2
S3_BUCKET=$3
aws s3 cp s3://$S3_BUCKET/$ENVIRONMENT/reassignment/reassignment.json /mnt/local_disk01/reassignment.json
/mnt/local_disk01/kafka/bin/kafka-reassign-partitions.sh \
  --bootstrap-server $BOOTSTRAP_SERVERS \
  --reassignment-json-file /mnt/local_disk01/reassignment.json \
  --execute \
  --command-config /mnt/local_disk01/kafka/config/ssl.properties
```

#### `scripts/verify_reassignment.sh`
```bash
#!/bin/bash
BOOTSTRAP_SERVERS=$1
ENVIRONMENT=$2
S3_BUCKET=$3
/mnt/local_disk01/kafka/bin/kafka-reassign-partitions.sh \
  --bootstrap-server $BOOTSTRAP_SERVERS \
  --reassignment-json-file /mnt/local_disk01/reassignment.json \
  --verify \
  --command-config /mnt/local_disk01/kafka/config/ssl.properties
```

#### `scripts/configure_monitoring.sh`
```bash
#!/bin/bash
S3_BUCKET=$1
ENVIRONMENT=$2
aws s3 cp s3://$S3_BUCKET/$ENVIRONMENT/monitoring/prometheus.yml /opt/prometheus/prometheus.yml
aws s3 cp s3://$S3_BUCKET/$ENVIRONMENT/monitoring/cloudwatch-agent.json /mnt/local_disk01/cloudwatch-agent.json
/opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/mnt/local_disk01/cloudwatch-agent.json -s
```

#### `scripts/push_metrics.sh`
```bash
#!/bin/bash
BOOTSTRAP_SERVERS=$1
ENVIRONMENT=$2
USERNAME=$(aws secretsmanager get-secret-value --secret-id ${ENVIRONMENT}-kafka-sasl-credentials --query SecretString --output text | jq -r '.username')
PASSWORD=$(aws secretsmanager get-secret-value --secret-id ${ENVIRONMENT}-kafka-sasl-credentials --query SecretString --output text | jq -r '.password')
cat <<EOT > /mnt/local_disk01/kafka/config/ssl.properties
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="$USERNAME" password="$PASSWORD";
ssl.truststore.location=/mnt/local_disk01/kafka/ssl/kafka.truststore.jks
ssl.keystore.location=/mnt/local_disk01/kafka/ssl/kafka.keystore.jks
EOT
BROKER_COUNT=$(/mnt/local_disk01/kafka/bin/kafka-broker-api-versions.sh --bootstrap-server $BOOTSTRAP_SERVERS --command-config /mnt/local_disk01/kafka/config/ssl.properties | grep -c broker)
aws cloudwatch put-metric-data --metric-name ActiveBrokerCount --namespace Kafka --value $BROKER_COUNT --dimensions Environment=$ENVIRONMENT
UNDER_REPLICATED=$(/mnt/local_disk01/kafka/bin/kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS --under-replicated-partitions --command-config /mnt/local_disk01/kafka/config/ssl.properties | wc -l)
aws cloudwatch put-metric-data --metric-name UnderReplicatedPartitions --namespace Kafka --value $UNDER_REPLICATED --dimensions Environment=$ENVIRONMENT
DISK_USED=$(df -h /mnt/local_disk02 | tail -1 | awk '{print $5}' | tr -d '%')
aws cloudwatch put-metric-data --metric-name DiskUsedPercent --namespace Kafka --value $DISK_USED --dimensions Environment=$ENVIRONMENT
LAG=$(/mnt/local_disk01/kafka/bin/kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS --describe --group test-group --command-config /mnt/local_disk01/kafka/config/ssl.properties | grep -v LAG | awk '{sum+=$6} END {print sum}')
aws cloudwatch put-metric-data --metric-name ConsumerLag --namespace Kafka --value $LAG --dimensions Environment=$ENVIRONMENT
```

#### `scripts/test_security.sh`
```bash
#!/bin/bash
BOOTSTRAP_SERVERS=$1
ENVIRONMENT=$2
SECRET_ID=$3
USERNAME=$(aws secretsmanager get-secret-value --secret-id $SECRET_ID --query SecretString --output text | jq -r '.username')
/mnt/local_disk01/kafka/bin/kafka-console-producer.sh --bootstrap-server $BOOTSTRAP_SERVERS --topic test-topic --producer.config /mnt/local_disk01/kafka/config/ssl.properties <<EOF
test-message
EOF
cat <<EOT > /mnt/local_disk01/kafka/config/invalid-ssl.properties
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required username="wrong" password="wrong";
ssl.truststore.location=/mnt/local_disk01/kafka/ssl/kafka.truststore.jks
ssl.keystore.location=/mnt/local_disk01/kafka/ssl/kafka.keystore.jks
EOT
if ! /mnt/local_disk01/kafka/bin/kafka-console-producer.sh --bootstrap-server $BOOTSTRAP_SERVERS --topic test-topic --producer.config /mnt/local_disk01/kafka/config/invalid-ssl.properties; then
  echo "Invalid credentials test passed"
else
  echo "Invalid credentials test failed"
fi
curl http://<monitoring_ip>:8000/v3/kafka/test-group | jq .
aws lambda invoke --function-name ${ENVIRONMENT}-kafka-reassign-partitions /tmp/lambda_output.json
cat /tmp/lambda_output.json | jq .
```

#### `scripts/test_alerting.sh`
```bash
#!/bin/bash
BOOTSTRAP_SERVERS=$1
ENVIRONMENT=$2
/mnt/local_disk01/kafka/bin/kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS --group test-group --reset-offsets --to-latest --execute --command-config /mnt/local_disk01/kafka/config/ssl.properties
sleep 60
aws cloudwatch describe-alarms --alarm-names "${ENVIRONMENT}-kafka-consumer-lag" | jq '.MetricAlarms[0].StateValue'
BROKER_ID=$(aws ec2 describe-instances --filters "Name=tag:Role,Values=broker" --query 'Reservations[0].Instances[0].InstanceId' --output text)
aws ec2 stop-instances --instance-ids $BROKER_ID
 sleep 60
aws cloudwatch describe-alarms --alarm-names "${ENVIRONMENT}-kafka-broker-offline" | jq '.MetricAlarms[0].StateValue'
/mnt/local_disk01/kafka/bin/kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS --alter --topic test-topic --replication-factor 1 --command-config /mnt/local_disk01/kafka/config/ssl.properties
sleep 60
aws cloudwatch describe-alarms --alarm-names "${ENVIRONMENT}-kafka-under-replicated" | jq '.MetricAlarms[0].StateValue'
aws cloudwatch describe-alarms --alarm-names "${ENVIRONMENT}-kafka-disk-usage" | jq '.MetricAlarms[0].StateValue'
```

#### `scripts/test_monitoring.sh`
```bash
#!/bin/bash
BOOTSTRAP_SERVERS=$1
MONITORING_IP=$2
curl http://$MONITORING_IP:9090/metrics | grep kafka_server
aws cloudwatch get-metric-statistics --namespace Kafka --metric-name ConsumerLag --start-time $(date -u -d "5 minutes ago" +%Y-%m-%dT%H:%M:%SZ) --end-time $(date -u +%Y-%m-%dT%H:%M:%SZ) --period 60 --statistics Average
curl http://$MONITORING_IP:8000/v3/kafka/test-group | jq .
curl http://$MONITORING_IP:3000/api/health
```

#### `scripts/test_reassignment.sh`
```bash
#!/bin/bash
BOOTSTRAP_SERVERS=$1
ENVIRONMENT=$2
S3_BUCKET=$3
./generate_reassignment.sh $BOOTSTRAP_SERVERS $ENVIRONMENT $S3_BUCKET
./execute_reassignment.sh $BOOTSTRAP_SERVERS $ENVIRONMENT $S3_BUCKET
./verify_reassignment.sh $BOOTSTRAP_SERVERS $ENVIRONMENT $S3_BUCKET
/mnt/local_disk01/kafka/bin/kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS --describe --command-config /mnt/local_disk01/kafka/config/ssl.properties
```

#### `scripts/test_connectivity.sh`
```bash
#!/bin/bash
BOOTSTRAP_SERVERS=$1
/mnt/local_disk01/kafka/bin/kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS --list --command-config /mnt/local_disk01/kafka/config/ssl.properties
```

#### `scripts/test_mirrormaker.sh`
```bash
#!/bin/bash
BOOTSTRAP_SERVERS=$1
/mnt/local_disk01/kafka/bin/kafka-console-consumer.sh --bootstrap-server $BOOTSTRAP_SERVERS --topic test-topic --from-beginning --consumer.config /mnt/local_disk01/kafka/config/ssl.properties
```

#### `scripts/validate_kafka.sh`
```bash
#!/bin/bash
BOOTSTRAP_SERVERS=$1
/mnt/local_disk01/kafka/bin/kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS --list --command-config /mnt/local_disk01/kafka/config/ssl.properties
```

### Properties Files

#### `properties/server.properties.tftpl`
```
broker.id=0
num.partitions=3
default.replication.factor=3
log.retention.hours=168
log.dirs=/mnt/local_disk02,/mnt/local_disk03,/mnt/local_disk04,/mnt/local_disk05,/mnt/local_disk06
listeners=SASL_SSL://0.0.0.0:9092
advertised.listeners=SASL_SSL://${hostname}:9092
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
```

#### `properties/controller.properties.tftpl`
```
node.id=0
process.roles=controller
listeners=SASL_SSL://0.0.0.0:9093
advertised.listeners=SASL_SSL://${hostname}:9093
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
```

#### `properties/mirrormaker.properties.tftpl`
```
bootstrap.servers=source-cluster:9092
source.cluster.alias=source
destination.cluster.alias=dest
source.cluster.bootstrap.servers=source-kafka:9092
dest.cluster.bootstrap.servers=dest-kafka:9092
security.protocol=SASL_SSL
sasl.mechanism=SCRAM-SHA-256
```

#### `properties/monitoring/prometheus.yml`
```yaml
global:
  scrape_interval: 15s
scrape_configs:
  - job_name: 'kafka'
    static_configs:
      - targets: [${join(",", [for ip in kafka_brokers : "\"${ip}:7071\""])}]
    metrics_path: /metrics
```

#### `properties/monitoring/cloudwatch-agent.json`
```json
{
  "metrics": {
    "namespace": "Kafka",
    "metrics_collected": [
      { "metric_name": "ConsumerLag", "metrics_path": "/mnt/local_disk01/push_metrics.sh" },
      { "metric_name": "BrokerRequestTime", "metrics_path": "/mnt/local_disk01/kafka/jmx-exporter.jar" }
    ]
  }
}
```

#### `properties/monitoring/burrow.toml`
```toml
[general]
loglevel="info"
[kafka]
client-id="burrow-client"
brokers=[${join(",", [for ip in kafka_controllers : "\"${ip}:9092\""])}]
sasl-mechanism="SCRAM-SHA-256"
security-protocol="SASL_SSL"
sasl-username="${USERNAME}"
sasl-password="${PASSWORD}"
```

### Environment Configuration

#### `environments/dev/terraform.tfvars`
```hcl
region        = "us-east-1"
environment   = "dev"
vpc_cidr      = "10.0.0.0/16"
azs           = ["us-east-1a", "us-east-1b", "us-east-1c"]
instance_type = "m5.xlarge"
monitoring_instance_type = "m5.large"
s3_bucket     = "my-kafka-bucket"
ami_id        = "ami-12345678" # Replace with actual AMI
key_name      = "my-key-pair"
sasl_username = "kafka-admin"
sasl_password = "secure-password" # Replace with actual password
alert_email   = "admin@example.com"
```

### Test Plan (`test/test_plan.md`)
```markdown
# Test Plan for Kafka KRaft Cluster

1. **Infrastructure Validation**:
   - Verify 12 Kafka nodes (9 brokers, 3 controllers) and 4 MirrorMaker nodes are running.
   - Check 6 EBS volumes (1TB each) are attached and mounted correctly.
   - Validate security group rules and VPC connectivity.

2. **Kafka Cluster Testing**:
   - Run `validate_kafka.sh` to check broker and controller status.
   - Create a test topic and produce/consume messages.
   - Verify SASL/SSL connectivity using `kafka-console-producer` and `kafka-console-consumer`.

3. **MirrorMaker Testing**:
   - Run `test_mirrormaker.sh` to confirm data replication between clusters.
   - Validate topic mirroring and data consistency.

4. **Failover Testing**:
   - Stop a controller node and verify cluster stability.
   - Simulate AZ failure and check recovery.

5. **Performance Testing**:
   - Use `kafka-producer-perf-test` and `kafka-consumer-perf-test` to measure throughput.

6. **Security Testing**:
   - Verify SASL/SSL authentication and encryption.
   - Run `test_security.sh` to test valid and invalid credentials.

7. **Autoscaling Testing**:
   - Simulate high CPU load using `stress` and verify scale-up.
   - Simulate consumer lag and check CloudWatch metric triggers.
   - Verify new brokers join the cluster and are assigned node IDs.
   - Scale down and confirm broker termination without data loss.

8. **Partition Reassignment Testing**:
   - Trigger a scale-up event and run `generate_reassignment.sh`.
   - Execute `execute_reassignment.sh` and confirm with `verify_reassignment.sh`.
   - Check partition distribution using `kafka-topics.sh --describe`.
   - Validate no data loss by producing/consuming messages.

9. **Monitoring Testing**:
   - Verify Prometheus scrapes JMX metrics (`curl <broker_ip>:7071/metrics`).
   - Check CloudWatch for `ConsumerLag` and `BrokerRequestTime` metrics.
   - Access Grafana UI (`http://<monitoring_ip>:3000`) and validate dashboards.
   - Run `test_monitoring.sh` to confirm Burrow reports consumer group health.

10. **Alerting Testing**:
    - Simulate high consumer lag and verify `ConsumerLag` alarm triggers.
    - Stop a broker and check `ActiveBrokerCount` alarm.
    - Simulate under-replicated partitions and confirm `UnderReplicatedPartitions` alarm.
    - Fill a disk to >90% and validate `DiskUsedPercent` alarm.
    - Test Lambda remediation and check logs.
    - Run `test_alerting.sh` to automate checks.

11. **Secrets Manager Testing**:
    - Verify brokers, controllers, MirrorMaker, and Burrow retrieve SASL credentials.
    - Test connectivity with Secrets Manager credentials.
    - Validate Lambda functions access Secrets Manager.
    - Run `test_secrets.sh` to automate checks.
```

### README (`README.md`)
```markdown:disable-run
# Kafka KRaft Cluster on AWS

## Overview
This Terraform configuration deploys a 12-node Kafka KRaft cluster (9 brokers, 3 controllers) with autoscaling, partition reassignment, monitoring, alerting, and SASL/SSL security using AWS Secrets Manager. It includes a 4-node MirrorMaker cluster, 6 EBS volumes (1TB each) per node, and supports dev, QA, staging, and prod environments.

## Prerequisites
- AWS account with CLI configured.
- Terraform >= 1.5.0.
- S3 bucket with Kafka 3.9.0 binaries, SSL certificates (`kafka.truststore.jks`, `kafka.keystore.jks`), and monitoring tools (`prometheus-2.45.0.tar.gz`, `jmx-exporter-0.16.1.jar`, `grafana-9.5.3.rpm`, `burrow-1.6.0.tar.gz`, `amazon-cloudwatch-agent.rpm`).
- SSH key pair.

## Deployment
1. Initialize Terraform:
   ```bash
   terraform init
   ```
2. Deploy to an environment:
   ```bash
   cd environments/dev
   terraform apply
   ```
3. Package and upload Lambda functions:
   ```bash
   cd
```